{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/ismayilzadamaharram/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/ismayilzadamaharram/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     /Users/ismayilzadamaharram/nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import h5py\n",
    "import pickle\n",
    "\n",
    "\n",
    "\n",
    "import gc\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score, classification_report, precision_score, recall_score\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.utils import resample\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from textblob import TextBlob\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from transformers import DistilBertTokenizer, DistilBertModel, TFDistilBertModel\n",
    "#from transformers import TFAutoModel, AutoTokenizer\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#from gensim.models import Word2Vec\n",
    "\n",
    "import joblib\n",
    "import xgboost as xgb\n",
    "import nltk\n",
    "import seaborn as sns\n",
    "#import shap\n",
    "import logging\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "\n",
    "from textstat import textstat\n",
    "from multiprocessing import Pool\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"]=\"TRUE\"\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('vader_lexicon')\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "###PREPROCESSING PART\n",
    "\n",
    "SYMBOLS_TO_ISOLATE = '.,?!-;*\"â€¦:â€”()%#$&_/@ï¼¼ãƒ»Ï‰+=â€â€œ[]^â€“>\\\\Â°<~â€¢â‰ â„¢ÊŠÉ’âˆžÂ§{}Â·Ï„Î±â¤â˜ºÉ¡|Â¢â†’Ì¶`â¥â”â”£â”«â”—ï¼¯â–ºâ˜…Â©â€•Éªâœ”Â®\\x96\\x92â—Â£â™¥âž¤Â´Â¹â˜•â‰ˆÃ·â™¡â—â•‘â–¬â€²É”Ëâ‚¬Û©Ûžâ€ Î¼âœ’âž¥â•â˜†ËŒâ—„Â½Ê»Ï€Î´Î·Î»ÏƒÎµÏÎ½Êƒâœ¬ï¼³ï¼µï¼°ï¼¥ï¼²ï¼©ï¼´â˜»Â±â™ÂµÂºÂ¾âœ“â—¾ØŸï¼Žâ¬…â„…Â»Ð’Ð°Ð²â£â‹…Â¿Â¬â™«ï¼£ï¼­Î²â–ˆâ–“â–’â–‘â‡’â­â€ºÂ¡â‚‚â‚ƒâ§â–°â–”â—žâ–€â–‚â–ƒâ–„â–…â–†â–‡â†™Î³Ì„â€³â˜¹âž¡Â«Ï†â…“â€žâœ‹ï¼šÂ¥Ì²Ì…Ìâˆ™â€›â—‡âœâ–·â“â—Â¶ËšË™ï¼‰ÑÐ¸Ê¿âœ¨ã€‚É‘\\x80â—•ï¼ï¼…Â¯âˆ’ï¬‚ï¬â‚Â²ÊŒÂ¼â´â„â‚„âŒ â™­âœ˜â•ªâ–¶â˜­âœ­â™ªâ˜”â˜ â™‚â˜ƒâ˜ŽâœˆâœŒâœ°â†â˜™â—‹â€£âš“å¹´âˆŽâ„’â–ªâ–™â˜â…›ï½ƒï½ï½“Ç€â„®Â¸ï½—â€šâˆ¼â€–â„³â„â†â˜¼â‹†Ê’âŠ‚ã€â…”Â¨Í¡à¹âš¾âš½Î¦Ã—Î¸ï¿¦ï¼Ÿï¼ˆâ„ƒâ©â˜®âš æœˆâœŠâŒâ­•â–¸â– â‡Œâ˜â˜‘âš¡â˜„Ç«â•­âˆ©â•®ï¼Œä¾‹ï¼žÊ•ÉÌ£Î”â‚€âœžâ”ˆâ•±â•²â–â–•â”ƒâ•°â–Šâ–‹â•¯â”³â”Šâ‰¥â˜’â†‘â˜É¹âœ…â˜›â™©â˜žï¼¡ï¼ªï¼¢â—”â—¡â†“â™€â¬†Ì±â„\\x91â €Ë¤â•šâ†ºâ‡¤âˆâœ¾â—¦â™¬Â³ã®ï½œï¼âˆµâˆ´âˆšÎ©Â¤â˜œâ–²â†³â–«â€¿â¬‡âœ§ï½ï½–ï½ï¼ï¼’ï¼ï¼˜ï¼‡â€°â‰¤âˆ•Ë†âšœâ˜'\n",
    "SYMBOLS_TO_REMOVE = '\\n\\r\\xa0\\ue014\\t\\uf818\\uf04a\\xad\\uf0e0\\u200b\\u200eØ¹Ø¯ÙˆÙŠÙ‡ØµÙ‚Ø£Ù†Ø§Ø®Ù„Ù‰Ø¨Ù…ØºØ±Ð•\\u202a\\u202cðŸ»á´ÊÊ€á´‡É´á´…á´á´€á´‹Êœá´œÊŸá´›á´„á´˜Ê™Ò“á´Šá´¡É¢×©×œ×•××‘×™â€¼\\x81ã‚¨ãƒ³ã‚¸æ•…éšœ\\u2009á´µÍž\\u200f××¢×›×—à®œá´ â€\\x7fá¼á½¶Î®Î¹á½²Îºá¼€Î¯á¿ƒá¼´Î¾ï¼¨\\ufeff\\u2028\\u3000ØªØ­ÙƒØ³Ø©ðŸ‘®ðŸ’™ÙØ²Ø·\\u2008ðŸ¾\\x08â€‘åœ°ç„è°·ÑƒÐ»ÐºÐ½ÐŸÐ¾ÐÐæ­Œèˆžä¼Ž×”Ð¼Ï…Ñ‚Ñ•â¤µ\\u200aÑÐ¿Ñ€Ð´\\x95\\u2002\\u2004Ë¢áµ’Ê³Ê¸á´¼á´·á´ºÊ·áµ—Ê°áµ‰áµ˜\\x13\\ue602Î¬Î¿ÏŒÏ‚Î­á½¸×ª×ž×“×£× ×¨×š×¦×˜\\uf0b7\\uf04c\\x9f\\x10æˆéƒ½\\u202dðŸ’¤ðŸ‡\\ue613å°åœŸè±†ðŸ¡â”â‰\\u202fðŸ‘ ã€‹à¤•à¤°à¥à¤®à¤¾ðŸ‡¹ðŸ‡¼ðŸŒ¸è”¡è‹±æ–‡ðŸŒžðŸŽ²ãƒ¬ã‚¯ã‚µã‚¹å¤–å›½äººå…³ç³»Ð¡Ð±ðŸ’‹ðŸ’€ðŸŽ„ðŸ’œÑŒÑ‹Ð³Ñä¸æ˜¯\\x9c\\x9dðŸ—‘\\u2005ðŸ’ƒðŸ“£à¼¼ã¤à¼½á¸·Ð—Ð·â–±Ñ†ï¿¼å–æ¸©å“¥åŽè®®ä¼šä¸‹é™ä½ å¤±åŽ»æ‰€æœ‰çš„é’±åŠ æ‹¿å¤§åç¨Žéª—å­ðŸãƒ„ðŸŽ…\\x85ðŸºØ¢Ø¥Ø´Ø¡ðŸŽµðŸŒŽÍŸá¼”æ²¹åˆ«å…‹ðŸ¤¡Ð¹\\u2003ðŸš€ðŸ¤´Ê²ÑˆÑ‡Ð˜ÐžÐ Ð¤Ð”Ð¯ÐœÑŽÐ¶ðŸ–‘á½á½»Ïç‰¹æ®Šä½œæˆ¦ç¾¤Ñ‰ðŸ’¨åœ†æ˜Žå›­×§â„\\u200dð“’ð“²ð“¿ð“µì•ˆì˜í•˜ì„¸ìš”Ð–Ñ™ÐšÑ›ðŸ€ðŸ˜«ðŸ¤¤á¿¦æˆ‘å‡ºç”Ÿåœ¨äº†å¯ä»¥è¯´æ™®é€šè¯æ±‰è¯­å¥½æžðŸŽ¼ðŸ•ºðŸ¸ðŸ¥‚ðŸ—½ðŸŽ‡ðŸŽŠðŸ†˜ðŸ¤ ðŸ‘©ðŸ–’ðŸšªå¤©ä¸€å®¶âš²\\u2006âš­âš†â¬­â¬¯â–æ–°âœ€â•ŒðŸ‡«ðŸ‡·ðŸ‡©ðŸ‡ªðŸ‡®ðŸ‡¬ðŸ‡§ðŸ˜·ðŸ‡¨ðŸ‡¦Ð¥Ð¨ðŸŒ\\x1fæ€é¸¡ç»™çŒ´çœ‹Êð—ªð—µð—²ð—»ð˜†ð—¼ð˜‚ð—¿ð—®ð—¹ð—¶ð˜‡ð—¯ð˜ð—°ð˜€ð˜…ð—½ð˜„ð—±ðŸ“ºÏ–\\u2000Ò¯Õ½á´¦áŽ¥Ò»Íº\\u2007Õ°\\u2001É©ï½™ï½…àµ¦ï½ŒÆ½ï½ˆð“ð¡ðžð«ð®ððšðƒðœð©ð­ð¢ð¨ð§Æ„á´¨×Ÿá‘¯à»Î¤á§à¯¦Ð†á´‘Üð¬ð°ð²ð›ð¦ð¯ð‘ð™ð£ð‡ð‚ð˜ðŸŽÔœÐ¢á—žà±¦ã€”áŽ«ð³ð”ð±ðŸ”ðŸ“ð…ðŸ‹ï¬ƒðŸ’˜ðŸ’“Ñ‘ð˜¥ð˜¯ð˜¶ðŸ’ðŸŒ‹ðŸŒ„ðŸŒ…ð™¬ð™–ð™¨ð™¤ð™£ð™¡ð™®ð™˜ð™ ð™šð™™ð™œð™§ð™¥ð™©ð™ªð™—ð™žð™ð™›ðŸ‘ºðŸ·â„‹ð€ð¥ðªðŸš¶ð™¢á¼¹ðŸ¤˜Í¦ðŸ’¸Ø¬íŒ¨í‹°ï¼·ð™‡áµ»ðŸ‘‚ðŸ‘ƒÉœðŸŽ«\\uf0a7Ð‘Ð£Ñ–ðŸš¢ðŸš‚àª—à«àªœàª°àª¾àª¤à«€á¿†ðŸƒð“¬ð“»ð“´ð“®ð“½ð“¼â˜˜ï´¾Ì¯ï´¿â‚½\\ue807ð‘»ð’†ð’ð’•ð’‰ð’“ð’–ð’‚ð’ð’…ð’”ð’Žð’—ð’ŠðŸ‘½ðŸ˜™\\u200cÐ›â€’ðŸŽ¾ðŸ‘¹âŽŒðŸ’â›¸å…¬å¯“å…»å® ç‰©å—ðŸ„ðŸ€ðŸš‘ðŸ¤·æ“ç¾Žð’‘ð’šð’ð‘´ðŸ¤™ðŸ’æ¬¢è¿Žæ¥åˆ°é˜¿æ‹‰æ–¯×¡×¤ð™«ðŸˆð’Œð™Šð™­ð™†ð™‹ð™ð˜¼ð™…ï·»ðŸ¦„å·¨æ”¶èµ¢å¾—ç™½é¬¼æ„¤æ€’è¦ä¹°é¢áº½ðŸš—ðŸ³ðŸðŸðŸ–ðŸ‘ðŸ•ð’„ðŸ—ð ð™„ð™ƒðŸ‘‡é”Ÿæ–¤æ‹·ð—¢ðŸ³ðŸ±ðŸ¬â¦ãƒžãƒ«ãƒãƒ‹ãƒãƒ­æ ªå¼ç¤¾â›·í•œêµ­ì–´ã„¸ã…“ë‹ˆÍœÊ–ð˜¿ð™”â‚µð’©â„¯ð’¾ð“ð’¶ð“‰ð“‡ð“Šð“ƒð“ˆð“…â„´ð’»ð’½ð“€ð“Œð’¸ð“Žð™Î¶ð™Ÿð˜ƒð—ºðŸ®ðŸ­ðŸ¯ðŸ²ðŸ‘‹ðŸ¦Šå¤šä¼¦ðŸ½ðŸŽ»ðŸŽ¹â›“ðŸ¹ðŸ·ðŸ¦†ä¸ºå’Œä¸­å‹è°Šç¥è´ºä¸Žå…¶æƒ³è±¡å¯¹æ³•å¦‚ç›´æŽ¥é—®ç”¨è‡ªå·±çŒœæœ¬ä¼ æ•™å£«æ²¡ç§¯å”¯è®¤è¯†åŸºç£å¾’æ›¾ç»è®©ç›¸ä¿¡è€¶ç¨£å¤æ´»æ­»æ€ªä»–ä½†å½“ä»¬èŠäº›æ”¿æ²»é¢˜æ—¶å€™æˆ˜èƒœå› åœ£æŠŠå…¨å ‚ç»“å©šå­©ææƒ§ä¸”æ —è°“è¿™æ ·è¿˜â™¾ðŸŽ¸ðŸ¤•ðŸ¤’â›‘ðŸŽæ‰¹åˆ¤æ£€è®¨ðŸðŸ¦ðŸ™‹ðŸ˜¶ì¥ìŠ¤íƒ±íŠ¸ë¤¼ë„ì„ìœ ê°€ê²©ì¸ìƒì´ê²½ì œí™©ì„ë µê²Œë§Œë“¤ì§€ì•Šë¡ìž˜ê´€ë¦¬í•´ì•¼í•©ë‹¤ìºë‚˜ì—ì„œëŒ€ë§ˆì´ˆì™€í™”ì•½ê¸ˆì˜í’ˆëŸ°ì„±ë¶„ê°ˆë•ŒëŠ”ë°˜ë“œì‹œí—ˆëœì‚¬ìš©ðŸ”«ðŸ‘å‡¸á½°ðŸ’²ðŸ—¯ð™ˆá¼Œð’‡ð’ˆð’˜ð’ƒð‘¬ð‘¶ð•¾ð–™ð–—ð–†ð–Žð–Œð–ð–•ð–Šð–”ð–‘ð–‰ð–“ð–ð–œð–žð–šð–‡ð•¿ð–˜ð–„ð–›ð–’ð–‹ð–‚ð•´ð–Ÿð–ˆð•¸ðŸ‘‘ðŸš¿ðŸ’¡çŸ¥å½¼ç™¾\\uf005ð™€ð’›ð‘²ð‘³ð‘¾ð’‹ðŸ’ðŸ˜¦ð™’ð˜¾ð˜½ðŸð˜©ð˜¨á½¼á¹‘ð‘±ð‘¹ð‘«ð‘µð‘ªðŸ‡°ðŸ‡µðŸ‘¾á“‡á’§á”­áƒá§á¦á‘³á¨á“ƒá“‚á‘²á¸á‘­á‘Žá“€á£ðŸ„ðŸŽˆðŸ”¨ðŸŽðŸ¤žðŸ¸ðŸ’ŸðŸŽ°ðŸŒðŸ›³ç‚¹å‡»æŸ¥ç‰ˆðŸ­ð‘¥ð‘¦ð‘§ï¼®ï¼§ðŸ‘£\\uf020ã£ðŸ‰Ñ„ðŸ’­ðŸŽ¥ÎžðŸ´ðŸ‘¨ðŸ¤³ðŸ¦\\x0bðŸ©ð‘¯ð’’ðŸðŸ‚ðŸ‘³ðŸ—ðŸ•‰ðŸ²Ú†ÛŒð‘®ð—•ð—´ðŸ’êœ¥â²£â²ðŸ‘â°é‰„ãƒªäº‹ä»¶Ñ—ðŸ’Šã€Œã€\\uf203\\uf09a\\uf222\\ue608\\uf202\\uf099\\uf469\\ue607\\uf410\\ue600ç‡»è£½ã‚·è™šå½å±ç†å±ˆÐ“ð‘©ð‘°ð’€ð‘ºðŸŒ¤ð—³ð—œð—™ð—¦ð—§ðŸŠá½ºá¼ˆá¼¡Ï‡á¿–Î›â¤ðŸ‡³ð’™ÏˆÕÕ´Õ¥Õ¼Õ¡ÕµÕ«Õ¶Ö€Ö‚Õ¤Õ±å†¬è‡³á½€ð’ðŸ”¹ðŸ¤šðŸŽð‘·ðŸ‚ðŸ’…ð˜¬ð˜±ð˜¸ð˜·ð˜ð˜­ð˜“ð˜–ð˜¹ð˜²ð˜«Ú©Î’ÏŽðŸ’¢ÎœÎŸÎÎ‘Î•ðŸ‡±â™²ðˆâ†´ðŸ’’âŠ˜È»ðŸš´ðŸ–•ðŸ–¤ðŸ¥˜ðŸ“ðŸ‘ˆâž•ðŸš«ðŸŽ¨ðŸŒ‘ðŸ»ðŽððŠð‘­ðŸ¤–ðŸŽŽðŸ˜¼ðŸ•·ï½‡ï½’ï½Žï½”ï½‰ï½„ï½•ï½†ï½‚ï½‹ðŸ°ðŸ‡´ðŸ‡­ðŸ‡»ðŸ‡²ð—žð—­ð—˜ð—¤ðŸ‘¼ðŸ“‰ðŸŸðŸ¦ðŸŒˆðŸ”­ã€ŠðŸŠðŸ\\uf10aáƒšÚ¡ðŸ¦\\U0001f92f\\U0001f92aðŸ¡ðŸ’³á¼±ðŸ™‡ð—¸ð—Ÿð— ð—·ðŸ¥œã•ã‚ˆã†ãªã‚‰ðŸ”¼'\n",
    "POSITIVE_EMOJI = 'ðŸ˜œðŸ˜ŽðŸ˜ðŸ’–ðŸ˜€ðŸ˜‚ðŸ˜„ðŸ˜‹ðŸ‘ðŸ˜ŠðŸ‘ðŸ˜ƒðŸ˜˜ðŸ‘ŒðŸ™‚ðŸ˜‰ðŸ˜ðŸŽ‰ðŸ˜…ðŸ‘»ðŸ™ƒðŸ˜†ðŸ¤—ðŸ¤“ðŸ˜ŒðŸ¤‘ðŸ˜›ðŸ¤£ðŸ˜ðŸ’ªðŸ˜—ðŸ¥°ðŸ˜‡ðŸ¤ ðŸ¤¡ðŸ¥³ðŸ¥´ðŸ¤©ðŸ˜ºðŸ˜¸ðŸ˜¹ðŸ˜»ðŸ˜½âœŒï¸ðŸ¤ŸðŸ¤˜'\n",
    "NEGATIVE_EMOJI = 'ðŸ˜¢ðŸ‘ŽðŸ˜±ðŸ˜³ðŸ˜§ðŸ™€ðŸ˜ðŸ˜•ðŸ˜®ðŸ˜–ðŸ˜ŸðŸ˜¡ðŸ˜ ðŸ˜¤ðŸ˜žðŸ˜­ðŸ˜¥ðŸ˜”ðŸ˜“ðŸ˜ªðŸ˜¨ðŸ˜©ðŸ™ðŸ˜µðŸ˜’ÍðŸ˜£ðŸ˜²ðŸ˜¯ðŸ¤¢ÙÙŽðŸ˜°ðŸ‘¿ðŸ‘¿ðŸ¤¥ðŸ˜¬ðŸ˜·ðŸ¤’ðŸ¤•ðŸ¤¯ðŸ¤¬ðŸ¥ºðŸ™€ðŸ˜¿ðŸ˜¾ðŸ–•ðŸ»ðŸ–•ðŸ¼'\n",
    "NEUTRAL_EMOJI = 'ðŸ¶ï¸ðŸ•ðŸµðŸ’µðŸ”¥ðŸ’¥ðŸšŒðŸŒŸðŸ’©ðŸ’¯â›½ðŸš„ðŸ¼ðŸš²ðŸ˜ˆðŸ™ðŸŽ¯ðŸŒ¹ðŸ’”ðŸ‘ŠðŸ™„â›ºðŸ¾ðŸ½ðŸŽ†ðŸ»âºðŸŒðŸ’žðŸš“ðŸ””ðŸ“šðŸ€ðŸ‘ðŸ½ðŸŽ¶ðŸŒºðŸ¤”ðŸ°ðŸ‡ðŸˆðŸ˜ºðŸŒðŸ”ðŸ®ðŸðŸ†ðŸ‘ðŸŒ®ðŸŒ¯ðŸ¤¦ðŸ™ˆðŸ˜´ðŸ†•ðŸ‘…ðŸ‘¥ðŸ‘„ðŸ”„ðŸ”¤ðŸ‘‰ðŸ‘¤ðŸ¤§ðŸ‘¶ðŸ‘²ðŸ”›ðŸŽ“ðŸ¿ðŸ‡ºðŸ‡¸ðŸŒ ðŸŸðŸ’«ðŸ’°ðŸš¬ðŸ’ŽðŸ±ðŸ™†ðŸ’•ð˜Šð˜¦ð˜³ð˜¢ð˜µð˜°ð˜¤ð˜ºð˜´ð˜ªð˜§ð˜®ð˜£ðŸ’—ðŸ’šðŸ¾ðŸ•ðŸ”—ðŸš½ðŸ†ðŸŽƒðŸ–ðŸ™…â›²ðŸ°ðŸ¤ðŸ‘†ðŸ™ŒðŸ’›ðŸ‘€ðŸ™ŠðŸ™‰'\n",
    "ISOLATE_DICT = {ord(c):' special symbol '.format(ord(c)) for c in SYMBOLS_TO_ISOLATE}\n",
    "REMOVE_DICT = {ord(c):'' for c in SYMBOLS_TO_REMOVE}\n",
    "NEUTRAL_EMOJI_DICT = {ord(c):' neutral emoji ' for c in NEUTRAL_EMOJI}\n",
    "POSITIVE_EMOJI_DICT = {ord(c):' positive emoji ' for c in POSITIVE_EMOJI}\n",
    "NEGATIVE_EMOJI_DICT = {ord(c):' negative emoji ' for c in NEGATIVE_EMOJI}\n",
    "CONTRACTION_MAPPING = {\"ain't\": \"is not\", \"aren't\": \"are not\",\"can't\": \"cannot\", \"'cause\": \"because\", \"could've\": \"could have\", \"couldn't\": \"could not\", \"didn't\": \"did not\",  \"doesn't\": \"does not\", \"don't\": \"do not\", \"hadn't\": \"had not\", \"hasn't\": \"has not\", \"haven't\": \"have not\", \"he'd\": \"he would\",\"he'll\": \"he will\", \"he's\": \"he is\", \"how'd\": \"how did\", \"how'd'y\": \"how do you\", \"how'll\": \"how will\", \"how's\": \"how is\",  \"I'd\": \"I would\", \"I'd've\": \"I would have\", \"I'll\": \"I will\", \"I'll've\": \"I will have\",\"I'm\": \"I am\", \"I've\": \"I have\", \"i'd\": \"i would\", \"i'd've\": \"i would have\", \"i'll\": \"i will\",  \"i'll've\": \"i will have\",\"i'm\": \"i am\", \"i've\": \"i have\", \"isn't\": \"is not\", \"it'd\": \"it would\", \"it'd've\": \"it would have\", \"it'll\": \"it will\", \"it'll've\": \"it will have\",\"it's\": \"it is\", \"let's\": \"let us\", \"ma'am\": \"madam\", \"mayn't\": \"may not\", \"might've\": \"might have\",\"mightn't\": \"might not\",\"mightn't've\": \"might not have\", \"must've\": \"must have\", \"mustn't\": \"must not\", \"mustn't've\": \"must not have\", \"needn't\": \"need not\", \"needn't've\": \"need not have\",\"o'clock\": \"of the clock\", \"oughtn't\": \"ought not\", \"oughtn't've\": \"ought not have\", \"shan't\": \"shall not\", \"sha'n't\": \"shall not\", \"shan't've\": \"shall not have\", \"she'd\": \"she would\", \"she'd've\": \"she would have\", \"she'll\": \"she will\", \"she'll've\": \"she will have\", \"she's\": \"she is\", \"should've\": \"should have\", \"shouldn't\": \"should not\", \"shouldn't've\": \"should not have\", \"so've\": \"so have\",\"so's\": \"so as\", \"this's\": \"this is\",\"that'd\": \"that would\", \"that'd've\": \"that would have\", \"that's\": \"that is\", \"there'd\": \"there would\", \"there'd've\": \"there would have\", \"there's\": \"there is\", \"here's\": \"here is\",\"they'd\": \"they would\", \"they'd've\": \"they would have\", \"they'll\": \"they will\", \"they'll've\": \"they will have\", \"they're\": \"they are\", \"they've\": \"they have\", \"to've\": \"to have\", \"wasn't\": \"was not\", \"we'd\": \"we would\", \"we'd've\": \"we would have\", \"we'll\": \"we will\", \"we'll've\": \"we will have\", \"we're\": \"we are\", \"we've\": \"we have\", \"weren't\": \"were not\", \"what'll\": \"what will\", \"what'll've\": \"what will have\", \"what're\": \"what are\",  \"what's\": \"what is\", \"what've\": \"what have\", \"when's\": \"when is\", \"when've\": \"when have\", \"where'd\": \"where did\", \"where's\": \"where is\", \"where've\": \"where have\", \"who'll\": \"who will\", \"who'll've\": \"who will have\", \"who's\": \"who is\", \"who've\": \"who have\", \"why's\": \"why is\", \"why've\": \"why have\", \"will've\": \"will have\", \"won't\": \"will not\", \"won't've\": \"will not have\", \"would've\": \"would have\", \"wouldn't\": \"would not\", \"wouldn't've\": \"would not have\", \"y'all\": \"you all\", \"y'all'd\": \"you all would\",\"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\"y'all've\": \"you all have\",\"you'd\": \"you would\", \"you'd've\": \"you would have\", \"you'll\": \"you will\", \"you'll've\": \"you will have\", \"you're\": \"you are\", \"you've\": \"you have\" }\n",
    "WORDS_TO_REMOVE = ['http', 'https', 'ya']\n",
    "\n",
    "\n",
    "def handle_punctuation(text):\n",
    "    text = text.translate(REMOVE_DICT)\n",
    "    text = text.translate(NEUTRAL_EMOJI_DICT)\n",
    "    text = text.translate(POSITIVE_EMOJI_DICT)\n",
    "    text = text.translate(NEGATIVE_EMOJI_DICT)\n",
    "    text = text.translate(ISOLATE_DICT)\n",
    "    return text\n",
    "\n",
    "def clean_contractions(text, mapping=CONTRACTION_MAPPING):\n",
    "    specials = [\"â€™\", \"â€˜\", \"Â´\", \"`\"]\n",
    "    for s in specials:\n",
    "        text = text.replace(s, \"'\")\n",
    "    text = ' '.join([mapping[t] if t in mapping else t for t in text.split(\" \")])\n",
    "    return text\n",
    "\n",
    "def text_general_reworking(text):\n",
    "    spaces = ['\\u200b', '\\u200e', '\\u202a', '\\u202c', '\\ufeff', '\\uf0d8', '\\u2061', '\\x10', '\\x7f', '\\x9d', '\\xad', '\\xa0']\n",
    "    for space in spaces:\n",
    "        text = text.replace(space, ' ')\n",
    "    text = re.sub('\\s+', ' ', text)\n",
    "    text = re.sub(r'\\.{2,}', ' ', text)\n",
    "    text = re.sub(r'(:\\s?\\)|:-\\)|\\(\\s?:|\\(-:|:\\'\\))', ' positive emoji ', text)\n",
    "    text = re.sub(r'(:\\s?D|:-D|x-?D|X-?D)', ' positive emoji ', text)\n",
    "    text = re.sub(r'(<3|:\\*)', ' positive emoji ', text)\n",
    "    text = re.sub(r'(;-?\\)|;-?D|\\(-?;)', ' positive emoji ', text)\n",
    "    text = re.sub(r'(:\\s?\\(|:-\\(|\\)\\s?:|\\)-:)', ' negative emoji ', text)\n",
    "    text = re.sub(r'(:,\\(|:\\'\\(|:\"\\()', ' negative emoji ', text)\n",
    "    text = re.sub(r'((www\\.[\\S]+)|(https?://[\\S]+))', ' link ', text)\n",
    "    text = re.sub(r'\\brt\\b', '', text)\n",
    "    text = re.sub(r'\\[math\\]', ' LaTex math ', text)\n",
    "    text = re.sub(r'\\[\\/math\\]', ' LaTex math ', text)\n",
    "    text = re.sub(r'\\\\', ' LaTex ', text)\n",
    "    text = re.sub(r'\\brt\\b', '', text)\n",
    "    text = re.sub(\"([^\\\"^\\'].\\s)(\\\")([A-Z,a-z?])\", r\"\\1\\3\", text)\n",
    "    text = re.sub(\"(\\')(.\\\")\", r\"\\2\", text)\n",
    "    text = text.lower()\n",
    "    if '-' in text:\n",
    "        text = text.replace('-', ' - ')\n",
    "    text.strip()\n",
    "    return text\n",
    "\n",
    "def preprocess_text(data):\n",
    "    if pd.isna(data):\n",
    "        return \"\"\n",
    "    #Original preprocessing\n",
    "    words = word_tokenize(data.lower())\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    filtered_words = [word for word in words if word.isalnum() and word not in stop_words]\n",
    "\n",
    "    #Remove a particular words\n",
    "    filtered_words = [word for word in filtered_words if word not in WORDS_TO_REMOVE]\n",
    "    processed_text = \" \".join(filtered_words)\n",
    "\n",
    "    # Additional preprocessing steps\n",
    "    processed_text = text_general_reworking(processed_text)\n",
    "    processed_text = handle_punctuation(processed_text)\n",
    "    processed_text = clean_contractions(processed_text)\n",
    "\n",
    "    return processed_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "###COMMON FEATURES\n",
    "def compute_text_length_features(df, text_column):\n",
    "    df['char_count'] = df[text_column].apply(len)\n",
    "    df['word_count'] = df[text_column].apply(lambda x: len(x.split()))\n",
    "    df['sentence_count'] = df[text_column].apply(lambda x: len([s for s in re.split(r'[.!?]', x) if s.strip() != '']))\n",
    "    df['avg_word_length'] = df['char_count'] / (df['word_count'] + 1) # avoid division by zero\n",
    "    return df\n",
    "\n",
    "\n",
    "def compute_sentiment_features(df, text_column):\n",
    "    sia = SentimentIntensityAnalyzer()\n",
    "    #df['sentiment'] = df[text_column].apply(lambda x: TextBlob(x).sentiment.polarity)\n",
    "    sentiments = df[text_column].apply(lambda  x: sia.polarity_scores(x))\n",
    "    df['sentiment_neg'] = sentiments.apply(lambda x: x['neg'])\n",
    "    df['sentiment_neu'] = sentiments.apply(lambda x: x['neu'])\n",
    "    df['sentiment_pos'] = sentiments.apply(lambda x: x['pos'])\n",
    "    df['sentiment_compound'] = sentiments.apply(lambda x: x['compound'])\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def compute_lexical_diversity(df, text_column):\n",
    "    df['lexical_diversity'] = df[text_column].apply(lambda x: len(set(x.split())) / (len(x.split()) + 1))\n",
    "    return df\n",
    "\n",
    "def compute_subjectivity(df, text_column):\n",
    "    df['subjectivity'] = df[text_column].apply(lambda x: TextBlob(x).sentiment.subjectivity)\n",
    "    return df\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Function to load troll data\n",
    "def load_merged_data(filepath):\n",
    "    try:\n",
    "        df = pd.read_csv(filepath, low_memory=False)\n",
    "        df['processed_content'] = df['text'].apply(preprocess_text)\n",
    "        df = compute_text_length_features(df, 'processed_content')\n",
    "        df = compute_sentiment_features(df, 'processed_content')\n",
    "\n",
    "        #df = compute_word2vec_features(df, 'processed_content')\n",
    "        #df = reduce_dimensionality(df, 'w2v_features')\n",
    "        \n",
    "        #df = compute_stylistic_features(df, 'processed_content')\n",
    "        #df = compute_linguistic_features(df, 'processed_content')\n",
    "        #df = compute_punctuation_features(df, 'processed_content')\n",
    "        df = compute_lexical_diversity(df, 'processed_content')\n",
    "        df = compute_subjectivity(df, 'processed_content')\n",
    "        #df = compute_ngrams_features(df, 'processed_content')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        #CALLING EXCLUSIVE FEATURE FUNCTIONS\n",
    "        #df = compute_interaction_features(df)\n",
    "        #df = compute_temporal_features(df)\n",
    "        \n",
    "        return df\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Failed to load troll data from {filepath}: {e}\")\n",
    "        raise\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "# Vectorize text data\n",
    "def vectorize_text(data):\n",
    "    tfidf_vectorizer = TfidfVectorizer(max_features=10000, stop_words='english')\n",
    "    X = tfidf_vectorizer.fit_transform(data)\n",
    "    return X, tfidf_vectorizer\n",
    "\n",
    "def normalize_features(df, feature_columns):\n",
    "    scaler = MinMaxScaler()\n",
    "    df[feature_columns] = scaler.fit_transform(df[feature_columns])\n",
    "    return df, scaler\n",
    "\n",
    "\n",
    "def combine_features(X_text, df, feature_columns):\n",
    "    X_numerical = df[feature_columns].values\n",
    "    \n",
    "    if X_text.shape[0] != X_numerical.shape[0]:\n",
    "        raise ValueError(f\"Mismatch in number of samples: X_text has {X_text.shape[0]} samples, while X_numerical has {X_numerical.shape[0]} samples.\")\n",
    "    \n",
    "    X = np.hstack((X_text, X_numerical))\n",
    "    return X\n",
    "\n",
    "\n",
    "#ANOTHER METHOD\n",
    "#K-Fold cross validation\n",
    "'''def train_and_evaluate(X, y):\n",
    "    classifier = xgb.XGBClassifier(use_label_encoder=False, eval_metric='logloss', reg_alpha=5, reg_lambda=5, max_depth=4, n_estimators=100)\n",
    "    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    accuracies = []\n",
    "    f1_scores = []\n",
    "    roc_aucs = []\n",
    "    precision_scores = []\n",
    "    recall_scores = []\n",
    "    roc_aucs = []\n",
    "\n",
    "    for train_index, test_index in skf.split(X, y):\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "        \n",
    "        classifier.fit(X_train, y_train)\n",
    "        y_pred = classifier.predict(X_test)\n",
    "        \n",
    "        accuracies.append(accuracy_score(y_test, y_pred))\n",
    "        f1_scores.append(f1_score(y_test, y_pred))\n",
    "        #precision_scores.append(precision_scores(y_test, y_pred))\n",
    "        roc_aucs.append(roc_auc_score(y_test, y_pred))\n",
    "        recall_scores.append(recall_score(y_test, y_pred))\n",
    "        roc_aucs.append(roc_auc_score(y_test, y_pred))\n",
    "        \n",
    "        print(\"Fold Results:\")\n",
    "        print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "        print(\"F1 Score:\", f1_score(y_test, y_pred))\n",
    "        print(\"Precision:\", precision_score(y_test, y_pred))\n",
    "        print(\"Recall:\", recall_score(y_test, y_pred))\n",
    "        print(\"ROC AUC:\", roc_auc_score(y_test, y_pred))\n",
    "        print(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n",
    "\n",
    "    print(\"Average Accuracy:\", np.mean(accuracies))\n",
    "    print(\"Average F1 Score:\", np.mean(f1_scores))\n",
    "    print(\"Average Precision:\", np.mean(precision_scores))\n",
    "    print(\"Average Recall:\", np.mean(recall_scores))\n",
    "    print(\"Average ROC AUC:\", np.mean(roc_aucs))\n",
    "\n",
    "    return classifier'''\n",
    "#K-fold without cross validation XGBOOST\n",
    "'''def train_and_evaluate(X, y):\n",
    "    # Split the data into training and test sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "    \n",
    "    # Init the classifier\n",
    "    classifier = xgb.XGBClassifier(use_label_encoder=False, eval_metric='logloss', reg_alpha=5, reg_lambda=5, max_depth=4, n_estimators=100)\n",
    "    \n",
    "    # Train the classifier\n",
    "    classifier.fit(X_train, y_train)\n",
    "    \n",
    "    # Predict on the test \n",
    "    y_pred = classifier.predict(X_test)\n",
    "    \n",
    "    # Print the metrics\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred)\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "    roc_auc = roc_auc_score(y_test, y_pred)\n",
    "    \n",
    "    print(\"Accuracy:\", accuracy)\n",
    "    print(\"F1 Score:\", f1)\n",
    "    print(\"Precision:\", precision)\n",
    "    print(\"Recall:\", recall)\n",
    "    print(\"ROC AUC:\", roc_auc)\n",
    "    print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))\n",
    "\n",
    "    return classifier'''\n",
    "\n",
    "\n",
    "#80/20\n",
    "'''def train_and_evaluate(X, y):\n",
    "    # Split the data into training and testing sets (80/20 split)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "    # Initialize the classifier\n",
    "    classifier = xgb.XGBClassifier(use_label_encoder=False, eval_metric='logloss', reg_alpha=5, reg_lambda=5, max_depth=4, n_estimators=100)\n",
    "    \n",
    "    # Train the classifier on the training set\n",
    "    classifier.fit(X_train, y_train)\n",
    "\n",
    "    # Predict on the test set\n",
    "    y_pred = classifier.predict(X_test)\n",
    "    \n",
    "    # Evaluate the classifier\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred)\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "    roc_auc = roc_auc_score(y_test, y_pred)\n",
    "    \n",
    "    print(\"Results on Test Set:\")\n",
    "    print(\"Accuracy:\", accuracy)\n",
    "    print(\"F1 Score:\", f1)\n",
    "    print(\"Precision:\", precision)\n",
    "    print(\"Recall:\", recall)\n",
    "    print(\"ROC AUC:\", roc_auc)\n",
    "    print(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n",
    "\n",
    "    return classifier'''\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#NAIVE BAYES\n",
    "'''def train_and_evaluate(X, y):\n",
    "    # Split the data into training and test sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "    \n",
    "    # Init the classifier\n",
    "    classifier = GaussianNB()\n",
    "    \n",
    "    # Train the classifier\n",
    "    classifier.fit(X_train, y_train)\n",
    "    \n",
    "    # Predict on the test \n",
    "    y_pred = classifier.predict(X_test)\n",
    "    \n",
    "    # Print the metrics\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred)\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "    roc_auc = roc_auc_score(y_test, y_pred)\n",
    "    \n",
    "    print(\"Accuracy:\", accuracy)\n",
    "    print(\"F1 Score:\", f1)\n",
    "    print(\"Precision:\", precision)\n",
    "    print(\"Recall:\", recall)\n",
    "    print(\"ROC AUC:\", roc_auc)\n",
    "    print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))\n",
    "\n",
    "    return classifier'''\n",
    "\n",
    "#KNN\n",
    "def train_and_evaluate(X, y):\n",
    "    # Split the data into training and test sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "    \n",
    "    # Init the classifier\n",
    "    classifier = KNeighborsClassifier(n_neighbors=5)  # You can adjust n_neighbors as needed\n",
    "    \n",
    "    # Train the classifier\n",
    "    classifier.fit(X_train, y_train)\n",
    "    \n",
    "    # Predict on the test \n",
    "    y_pred = classifier.predict(X_test)\n",
    "    \n",
    "    # Print the metrics\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred)\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "    roc_auc = roc_auc_score(y_test, y_pred)\n",
    "    \n",
    "    print(\"Accuracy:\", accuracy)\n",
    "    print(\"F1 Score:\", f1)\n",
    "    print(\"Precision:\", precision)\n",
    "    print(\"Recall:\", recall)\n",
    "    print(\"ROC AUC:\", roc_auc)\n",
    "    print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))\n",
    "\n",
    "    return classifier\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class distribution:\n",
      "1    50000\n",
      "0    50000\n",
      "Name: label, dtype: int64\n",
      "Accuracy: 0.72815\n",
      "F1 Score: 0.731307141092167\n",
      "Precision: 0.7229115779189057\n",
      "Recall: 0.7399\n",
      "ROC AUC: 0.72815\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.73      0.72      0.72     10000\n",
      "           1       0.72      0.74      0.73     10000\n",
      "\n",
      "    accuracy                           0.73     20000\n",
      "   macro avg       0.73      0.73      0.73     20000\n",
      "weighted avg       0.73      0.73      0.73     20000\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"with open('xgb_model.pkl', 'wb') as model_file:\\n    pickle.dump(classifier, model_file)\\nwith open('scaler.pkl', 'wb') as scaler_file:\\n    pickle.dump(scaler, scaler_file)\""
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Execution block\n",
    "merged_file = \"../Merged_troll_non_troll/merged_dataset100000.csv\"\n",
    "\n",
    "\n",
    "# Load troll and non-troll datasets\n",
    "combined_df = load_merged_data(merged_file)\n",
    "\n",
    "\n",
    "\n",
    "# Investigate class distribution\n",
    "print(\"Class distribution:\")\n",
    "print(combined_df['label'].value_counts())\n",
    "\n",
    "# Define text and numerical feature columns\n",
    "\n",
    "numerical_features = ['char_count', 'word_count', 'sentence_count', 'avg_word_length',\n",
    "                      'sentiment_neg', 'sentiment_neu', 'sentiment_pos', 'sentiment_compound',\n",
    "                      'lexical_diversity', 'subjectivity']\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Handle NaN values if present by filling with zeros\n",
    "combined_df[numerical_features] = combined_df[numerical_features].fillna(0)\n",
    "\n",
    "\n",
    "#Normalize num features\n",
    "combined_df, scaler = normalize_features(combined_df, numerical_features)\n",
    "\n",
    "#Combine text and num features\n",
    "X = combined_df[numerical_features].values\n",
    "y = combined_df['label'].values\n",
    "\n",
    "\n",
    "classifier = train_and_evaluate(X, y)\n",
    "\n",
    "'''with open('xgb_model.pkl', 'wb') as model_file:\n",
    "    pickle.dump(classifier, model_file)\n",
    "with open('scaler.pkl', 'wb') as scaler_file:\n",
    "    pickle.dump(scaler, scaler_file)'''\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "72229e2cb4a4d3f9a66c0f3bbf8721a4f899e8fc91e357b565a53efd017c27d6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
