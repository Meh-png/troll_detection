{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/ismayilzadamaharram/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/ismayilzadamaharram/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     /Users/ismayilzadamaharram/nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import h5py\n",
    "import pickle\n",
    "\n",
    "\n",
    "\n",
    "import gc\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score, classification_report, precision_score, recall_score, confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.utils import resample\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from textblob import TextBlob\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from transformers import DistilBertTokenizer, DistilBertModel, TFDistilBertModel\n",
    "#from transformers import TFAutoModel, AutoTokenizer\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#from gensim.models import Word2Vec\n",
    "\n",
    "import joblib\n",
    "import xgboost as xgb\n",
    "import nltk\n",
    "import seaborn as sns\n",
    "#import shap\n",
    "import logging\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "\n",
    "from textstat import textstat\n",
    "from multiprocessing import Pool\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"]=\"TRUE\"\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('vader_lexicon')\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "###PREPROCESSING PART\n",
    "\n",
    "SYMBOLS_TO_ISOLATE = '.,?!-;*\"…:—()%#$&_/@＼・ω+=”“[]^–>\\\\°<~•≠™ʊɒ∞§{}·τα❤☺ɡ|¢→̶`❥━┣┫┗Ｏ►★©―ɪ✔®\\x96\\x92●£♥➤´¹☕≈÷♡◐║▬′ɔː€۩۞†μ✒➥═☆ˌ◄½ʻπδηλσερνʃ✬ＳＵＰＥＲＩＴ☻±♍µº¾✓◾؟．⬅℅»Вав❣⋅¿¬♫ＣＭβ█▓▒░⇒⭐›¡₂₃❧▰▔◞▀▂▃▄▅▆▇↙γ̄″☹➡«φ⅓„✋：¥̲̅́∙‛◇✏▷❓❗¶˚˙）сиʿ✨。ɑ\\x80◕！％¯−ﬂﬁ₁²ʌ¼⁴⁄₄⌠♭✘╪▶☭✭♪☔☠♂☃☎✈✌✰❆☙○‣⚓年∎ℒ▪▙☏⅛ｃａｓǀ℮¸ｗ‚∼‖ℳ❄←☼⋆ʒ⊂、⅔¨͡๏⚾⚽Φ×θ￦？（℃⏩☮⚠月✊❌⭕▸■⇌☐☑⚡☄ǫ╭∩╮，例＞ʕɐ̣Δ₀✞┈╱╲▏▕┃╰▊▋╯┳┊≥☒↑☝ɹ✅☛♩☞ＡＪＢ◔◡↓♀⬆̱ℏ\\x91⠀ˤ╚↺⇤∏✾◦♬³の｜／∵∴√Ω¤☜▲↳▫‿⬇✧ｏｖｍ－２０８＇‰≤∕ˆ⚜☁'\n",
    "SYMBOLS_TO_REMOVE = '\\n\\r\\xa0\\ue014\\t\\uf818\\uf04a\\xad\\uf0e0\\u200b\\u200eعدويهصقأناخلىبمغرЕ\\u202a\\u202c🏻ᴍʏʀᴇɴᴅᴏᴀᴋʜᴜʟᴛᴄᴘʙғᴊᴡɢשלוםבי‼\\x81エンジ故障\\u2009ᴵ͞\\u200fאעכחஜᴠ‐\\x7fἐὶήιὲκἀίῃἴξＨ\\ufeff\\u2028\\u3000تحكسة👮💙فزط\\u2008🏾\\x08‑地獄谷улкнПоАН歌舞伎הмυтѕ⤵\\u200aэпрд\\x95\\u2002\\u2004ˢᵒʳʸᴼᴷᴺʷᵗʰᵉᵘ\\x13\\ue602άοόςέὸתמדףנרךצט\\uf0b7\\uf04c\\x9f\\x10成都\\u202d💤🍇\\ue613小土豆🏡❔⁉\\u202f👠》कर्मा🇹🇼🌸蔡英文🌞🎲レクサス外国人关系Сб💋💀🎄💜ьыгя不是\\x9c\\x9d🗑\\u2005💃📣༼つ༽ḷЗз▱ц￼卖温哥华议会下降你失去所有的钱加拿大坏税骗子🐝ツ🎅\\x85🍺آإشء🎵🌎͟ἔ油别克🤡й\\u2003🚀🤴ʲшчИОРФДЯМюж🖑ὐύύ特殊作戦群щ💨圆明园קℐ\\u200d𝓒𝓲𝓿𝓵안영하세요ЖљКћ🍀😫🤤ῦ我出生在了可以说普通话汉语好极🎼🕺🍸🥂🗽🎇🎊🆘🤠👩🖒🚪天一家⚲\\u2006⚭⚆⬭⬯⏖新✀╌🇫🇷🇩🇪🇮🇬🇧😷🇨🇦ХШ🌐\\x1f杀鸡给猴看ʁ𝗪𝗵𝗲𝗻𝘆𝗼𝘂𝗿𝗮𝗹𝗶𝘇𝗯𝘁𝗰𝘀𝘅𝗽𝘄𝗱📺ϖ\\u2000үսᴦᎥһͺ\\u2007հ\\u2001ɩｙｅ൦ｌƽｈ𝐓𝐡𝐞𝐫𝐮𝐝𝐚𝐃𝐜𝐩𝐭𝐢𝐨𝐧Ƅᴨןᑯ໐ΤᏧ௦Іᴑ܁𝐬𝐰𝐲𝐛𝐦𝐯𝐑𝐙𝐣𝐇𝐂𝐘𝟎ԜТᗞ౦〔Ꭻ𝐳𝐔𝐱𝟔𝟓𝐅🐋ﬃ💘💓ё𝘥𝘯𝘶💐🌋🌄🌅𝙬𝙖𝙨𝙤𝙣𝙡𝙮𝙘𝙠𝙚𝙙𝙜𝙧𝙥𝙩𝙪𝙗𝙞𝙝𝙛👺🐷ℋ𝐀𝐥𝐪🚶𝙢Ἱ🤘ͦ💸ج패티Ｗ𝙇ᵻ👂👃ɜ🎫\\uf0a7БУі🚢🚂ગુજરાતીῆ🏃𝓬𝓻𝓴𝓮𝓽𝓼☘﴾̯﴿₽\\ue807𝑻𝒆𝒍𝒕𝒉𝒓𝒖𝒂𝒏𝒅𝒔𝒎𝒗𝒊👽😙\\u200cЛ‒🎾👹⎌🏒⛸公寓养宠物吗🏄🐀🚑🤷操美𝒑𝒚𝒐𝑴🤙🐒欢迎来到阿拉斯ספ𝙫🐈𝒌𝙊𝙭𝙆𝙋𝙍𝘼𝙅ﷻ🦄巨收赢得白鬼愤怒要买额ẽ🚗🐳𝟏𝐟𝟖𝟑𝟕𝒄𝟗𝐠𝙄𝙃👇锟斤拷𝗢𝟳𝟱𝟬⦁マルハニチロ株式社⛷한국어ㄸㅓ니͜ʖ𝘿𝙔₵𝒩ℯ𝒾𝓁𝒶𝓉𝓇𝓊𝓃𝓈𝓅ℴ𝒻𝒽𝓀𝓌𝒸𝓎𝙏ζ𝙟𝘃𝗺𝟮𝟭𝟯𝟲👋🦊多伦🐽🎻🎹⛓🏹🍷🦆为和中友谊祝贺与其想象对法如直接问用自己猜本传教士没积唯认识基督徒曾经让相信耶稣复活死怪他但当们聊些政治题时候战胜因圣把全堂结婚孩恐惧且栗谓这样还♾🎸🤕🤒⛑🎁批判检讨🏝🦁🙋😶쥐스탱트뤼도석유가격인상이경제황을렵게만들지않록잘관리해야합다캐나에서대마초와화약금의품런성분갈때는반드시허된사용🔫👁凸ὰ💲🗯𝙈Ἄ𝒇𝒈𝒘𝒃𝑬𝑶𝕾𝖙𝖗𝖆𝖎𝖌𝖍𝖕𝖊𝖔𝖑𝖉𝖓𝖐𝖜𝖞𝖚𝖇𝕿𝖘𝖄𝖛𝖒𝖋𝖂𝕴𝖟𝖈𝕸👑🚿💡知彼百\\uf005𝙀𝒛𝑲𝑳𝑾𝒋𝟒😦𝙒𝘾𝘽🏐𝘩𝘨ὼṑ𝑱𝑹𝑫𝑵𝑪🇰🇵👾ᓇᒧᔭᐃᐧᐦᑳᐨᓃᓂᑲᐸᑭᑎᓀᐣ🐄🎈🔨🐎🤞🐸💟🎰🌝🛳点击查版🍭𝑥𝑦𝑧ＮＧ👣\\uf020っ🏉ф💭🎥Ξ🐴👨🤳🦍\\x0b🍩𝑯𝒒𝟐🏂👳🍗🕉🐲چی𝑮𝗕𝗴🍒ꜥⲣⲏ🐑⏰鉄リ事件ї💊「」\\uf203\\uf09a\\uf222\\ue608\\uf202\\uf099\\uf469\\ue607\\uf410\\ue600燻製シ虚偽屁理屈Г𝑩𝑰𝒀𝑺🌤𝗳𝗜𝗙𝗦𝗧🍊ὺἈἡχῖΛ⤏🇳𝒙ψՁմեռայինրւդձ冬至ὀ𝒁🔹🤚🍎𝑷🐂💅𝘬𝘱𝘸𝘷𝘐𝘭𝘓𝘖𝘹𝘲𝘫کΒώ💢ΜΟΝΑΕ🇱♲𝝈↴💒⊘Ȼ🚴🖕🖤🥘📍👈➕🚫🎨🌑🐻𝐎𝐍𝐊𝑭🤖🎎😼🕷ｇｒｎｔｉｄｕｆｂｋ𝟰🇴🇭🇻🇲𝗞𝗭𝗘𝗤👼📉🍟🍦🌈🔭《🐊🐍\\uf10aლڡ🐦\\U0001f92f\\U0001f92a🐡💳ἱ🙇𝗸𝗟𝗠𝗷🥜さようなら🔼'\n",
    "POSITIVE_EMOJI = '😜😎😁💖😀😂😄😋👏😊👍😃😘👌🙂😉😏🎉😅👻🙃😆🤗🤓😌🤑😛🤣😝💪😗🥰😇🤠🤡🥳🥴🤩😺😸😹😻😽✌️🤟🤘'\n",
    "NEGATIVE_EMOJI = '😢👎😱😳😧🙀😐😕😮😖😟😡😠😤😞😭😥😔😓😪😨😩🙁😵😒͝😣😲😯🤢َِ😰👿👿🤥😬😷🤒🤕🤯🤬🥺🙀😿😾🖕🏻🖕🏼'\n",
    "NEUTRAL_EMOJI = '🐶️🍕🐵💵🔥💥🚌🌟💩💯⛽🚄🏼🚲😈🙏🎯🌹💔👊🙄⛺🍾🏽🎆🍻⏺🌏💞🚓🔔📚🏀👐🍽🎶🌺🤔🐰🐇🏈😺🌍🍔🐮🍁🍆🍑🌮🌯🤦🙈😴🆕👅👥👄🔄🔤👉👤🤧👶👲🔛🎓🏿🇺🇸🌠🐟💫💰🚬💎🐱🙆💕𝘊𝘦𝘳𝘢𝘵𝘰𝘤𝘺𝘴𝘪𝘧𝘮𝘣💗💚🐾🐕🔗🚽🏆🎃🖐🙅⛲🍰🤐👆🙌💛👀🙊🙉'\n",
    "ISOLATE_DICT = {ord(c):' special symbol '.format(ord(c)) for c in SYMBOLS_TO_ISOLATE}\n",
    "REMOVE_DICT = {ord(c):'' for c in SYMBOLS_TO_REMOVE}\n",
    "NEUTRAL_EMOJI_DICT = {ord(c):' neutral emoji ' for c in NEUTRAL_EMOJI}\n",
    "POSITIVE_EMOJI_DICT = {ord(c):' positive emoji ' for c in POSITIVE_EMOJI}\n",
    "NEGATIVE_EMOJI_DICT = {ord(c):' negative emoji ' for c in NEGATIVE_EMOJI}\n",
    "CONTRACTION_MAPPING = {\"ain't\": \"is not\", \"aren't\": \"are not\",\"can't\": \"cannot\", \"'cause\": \"because\", \"could've\": \"could have\", \"couldn't\": \"could not\", \"didn't\": \"did not\",  \"doesn't\": \"does not\", \"don't\": \"do not\", \"hadn't\": \"had not\", \"hasn't\": \"has not\", \"haven't\": \"have not\", \"he'd\": \"he would\",\"he'll\": \"he will\", \"he's\": \"he is\", \"how'd\": \"how did\", \"how'd'y\": \"how do you\", \"how'll\": \"how will\", \"how's\": \"how is\",  \"I'd\": \"I would\", \"I'd've\": \"I would have\", \"I'll\": \"I will\", \"I'll've\": \"I will have\",\"I'm\": \"I am\", \"I've\": \"I have\", \"i'd\": \"i would\", \"i'd've\": \"i would have\", \"i'll\": \"i will\",  \"i'll've\": \"i will have\",\"i'm\": \"i am\", \"i've\": \"i have\", \"isn't\": \"is not\", \"it'd\": \"it would\", \"it'd've\": \"it would have\", \"it'll\": \"it will\", \"it'll've\": \"it will have\",\"it's\": \"it is\", \"let's\": \"let us\", \"ma'am\": \"madam\", \"mayn't\": \"may not\", \"might've\": \"might have\",\"mightn't\": \"might not\",\"mightn't've\": \"might not have\", \"must've\": \"must have\", \"mustn't\": \"must not\", \"mustn't've\": \"must not have\", \"needn't\": \"need not\", \"needn't've\": \"need not have\",\"o'clock\": \"of the clock\", \"oughtn't\": \"ought not\", \"oughtn't've\": \"ought not have\", \"shan't\": \"shall not\", \"sha'n't\": \"shall not\", \"shan't've\": \"shall not have\", \"she'd\": \"she would\", \"she'd've\": \"she would have\", \"she'll\": \"she will\", \"she'll've\": \"she will have\", \"she's\": \"she is\", \"should've\": \"should have\", \"shouldn't\": \"should not\", \"shouldn't've\": \"should not have\", \"so've\": \"so have\",\"so's\": \"so as\", \"this's\": \"this is\",\"that'd\": \"that would\", \"that'd've\": \"that would have\", \"that's\": \"that is\", \"there'd\": \"there would\", \"there'd've\": \"there would have\", \"there's\": \"there is\", \"here's\": \"here is\",\"they'd\": \"they would\", \"they'd've\": \"they would have\", \"they'll\": \"they will\", \"they'll've\": \"they will have\", \"they're\": \"they are\", \"they've\": \"they have\", \"to've\": \"to have\", \"wasn't\": \"was not\", \"we'd\": \"we would\", \"we'd've\": \"we would have\", \"we'll\": \"we will\", \"we'll've\": \"we will have\", \"we're\": \"we are\", \"we've\": \"we have\", \"weren't\": \"were not\", \"what'll\": \"what will\", \"what'll've\": \"what will have\", \"what're\": \"what are\",  \"what's\": \"what is\", \"what've\": \"what have\", \"when's\": \"when is\", \"when've\": \"when have\", \"where'd\": \"where did\", \"where's\": \"where is\", \"where've\": \"where have\", \"who'll\": \"who will\", \"who'll've\": \"who will have\", \"who's\": \"who is\", \"who've\": \"who have\", \"why's\": \"why is\", \"why've\": \"why have\", \"will've\": \"will have\", \"won't\": \"will not\", \"won't've\": \"will not have\", \"would've\": \"would have\", \"wouldn't\": \"would not\", \"wouldn't've\": \"would not have\", \"y'all\": \"you all\", \"y'all'd\": \"you all would\",\"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\"y'all've\": \"you all have\",\"you'd\": \"you would\", \"you'd've\": \"you would have\", \"you'll\": \"you will\", \"you'll've\": \"you will have\", \"you're\": \"you are\", \"you've\": \"you have\" }\n",
    "WORDS_TO_REMOVE = ['http', 'https', 'ya']\n",
    "\n",
    "\n",
    "def handle_punctuation(text):\n",
    "    text = text.translate(REMOVE_DICT)\n",
    "    text = text.translate(NEUTRAL_EMOJI_DICT)\n",
    "    text = text.translate(POSITIVE_EMOJI_DICT)\n",
    "    text = text.translate(NEGATIVE_EMOJI_DICT)\n",
    "    text = text.translate(ISOLATE_DICT)\n",
    "    return text\n",
    "\n",
    "def clean_contractions(text, mapping=CONTRACTION_MAPPING):\n",
    "    specials = [\"’\", \"‘\", \"´\", \"`\"]\n",
    "    for s in specials:\n",
    "        text = text.replace(s, \"'\")\n",
    "    text = ' '.join([mapping[t] if t in mapping else t for t in text.split(\" \")])\n",
    "    return text\n",
    "\n",
    "def text_general_reworking(text):\n",
    "    spaces = ['\\u200b', '\\u200e', '\\u202a', '\\u202c', '\\ufeff', '\\uf0d8', '\\u2061', '\\x10', '\\x7f', '\\x9d', '\\xad', '\\xa0']\n",
    "    for space in spaces:\n",
    "        text = text.replace(space, ' ')\n",
    "    text = re.sub('\\s+', ' ', text)\n",
    "    text = re.sub(r'\\.{2,}', ' ', text)\n",
    "    text = re.sub(r'(:\\s?\\)|:-\\)|\\(\\s?:|\\(-:|:\\'\\))', ' positive emoji ', text)\n",
    "    text = re.sub(r'(:\\s?D|:-D|x-?D|X-?D)', ' positive emoji ', text)\n",
    "    text = re.sub(r'(<3|:\\*)', ' positive emoji ', text)\n",
    "    text = re.sub(r'(;-?\\)|;-?D|\\(-?;)', ' positive emoji ', text)\n",
    "    text = re.sub(r'(:\\s?\\(|:-\\(|\\)\\s?:|\\)-:)', ' negative emoji ', text)\n",
    "    text = re.sub(r'(:,\\(|:\\'\\(|:\"\\()', ' negative emoji ', text)\n",
    "    text = re.sub(r'((www\\.[\\S]+)|(https?://[\\S]+))', ' link ', text)\n",
    "    text = re.sub(r'\\brt\\b', '', text)\n",
    "    text = re.sub(r'\\[math\\]', ' LaTex math ', text)\n",
    "    text = re.sub(r'\\[\\/math\\]', ' LaTex math ', text)\n",
    "    text = re.sub(r'\\\\', ' LaTex ', text)\n",
    "    text = re.sub(r'\\brt\\b', '', text)\n",
    "    text = re.sub(\"([^\\\"^\\'].\\s)(\\\")([A-Z,a-z?])\", r\"\\1\\3\", text)\n",
    "    text = re.sub(\"(\\')(.\\\")\", r\"\\2\", text)\n",
    "    text = text.lower()\n",
    "    if '-' in text:\n",
    "        text = text.replace('-', ' - ')\n",
    "    text.strip()\n",
    "    return text\n",
    "\n",
    "def preprocess_text(data):\n",
    "    if pd.isna(data):\n",
    "        return \"\"\n",
    "    #Original preprocessing\n",
    "    words = word_tokenize(data.lower())\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    filtered_words = [word for word in words if word.isalnum() and word not in stop_words]\n",
    "\n",
    "    #Remove a particular words\n",
    "    filtered_words = [word for word in filtered_words if word not in WORDS_TO_REMOVE]\n",
    "    processed_text = \" \".join(filtered_words)\n",
    "\n",
    "    # Additional preprocessing steps\n",
    "    processed_text = text_general_reworking(processed_text)\n",
    "    processed_text = handle_punctuation(processed_text)\n",
    "    processed_text = clean_contractions(processed_text)\n",
    "\n",
    "    return processed_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "###COMMON FEATURES\n",
    "def compute_text_length_features(df, text_column):\n",
    "    df['char_count'] = df[text_column].apply(len)\n",
    "    df['word_count'] = df[text_column].apply(lambda x: len(x.split()))\n",
    "    df['sentence_count'] = df[text_column].apply(lambda x: len([s for s in re.split(r'[.!?]', x) if s.strip() != '']))\n",
    "    df['avg_word_length'] = df['char_count'] / (df['word_count'] + 1) # avoid division by zero\n",
    "    return df\n",
    "\n",
    "\n",
    "def compute_sentiment_features(df, text_column):\n",
    "    sia = SentimentIntensityAnalyzer()\n",
    "    #df['sentiment'] = df[text_column].apply(lambda x: TextBlob(x).sentiment.polarity)\n",
    "    sentiments = df[text_column].apply(lambda  x: sia.polarity_scores(x))\n",
    "    df['sentiment_neg'] = sentiments.apply(lambda x: x['neg'])\n",
    "    df['sentiment_neu'] = sentiments.apply(lambda x: x['neu'])\n",
    "    df['sentiment_pos'] = sentiments.apply(lambda x: x['pos'])\n",
    "    df['sentiment_compound'] = sentiments.apply(lambda x: x['compound'])\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def compute_lexical_diversity(df, text_column):\n",
    "    df['lexical_diversity'] = df[text_column].apply(lambda x: len(set(x.split())) / (len(x.split()) + 1))\n",
    "    return df\n",
    "\n",
    "def compute_subjectivity(df, text_column):\n",
    "    df['subjectivity'] = df[text_column].apply(lambda x: TextBlob(x).sentiment.subjectivity)\n",
    "    return df\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Function to load troll data\n",
    "def load_merged_data(filepath):\n",
    "    try:\n",
    "        df = pd.read_csv(filepath, low_memory=False)\n",
    "        df['processed_content'] = df['text'].apply(preprocess_text)\n",
    "        df = compute_text_length_features(df, 'processed_content')\n",
    "        df = compute_sentiment_features(df, 'processed_content')\n",
    "\n",
    "        #df = compute_word2vec_features(df, 'processed_content')\n",
    "        #df = reduce_dimensionality(df, 'w2v_features')\n",
    "        \n",
    "        #df = compute_stylistic_features(df, 'processed_content')\n",
    "        #df = compute_linguistic_features(df, 'processed_content')\n",
    "        #df = compute_punctuation_features(df, 'processed_content')\n",
    "        df = compute_lexical_diversity(df, 'processed_content')\n",
    "        df = compute_subjectivity(df, 'processed_content')\n",
    "        #df = compute_ngrams_features(df, 'processed_content')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        #CALLING EXCLUSIVE FEATURE FUNCTIONS\n",
    "        #df = compute_interaction_features(df)\n",
    "        #df = compute_temporal_features(df)\n",
    "        \n",
    "        return df\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Failed to load troll data from {filepath}: {e}\")\n",
    "        raise\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "###DISTILBERT NLP\n",
    "def data_generator(data, batch_size):\n",
    "    for start_idx in range(0, len(data), batch_size):\n",
    "        end_idx = min(start_idx + batch_size, len(data))\n",
    "        yield data[start_idx:end_idx]\n",
    "\n",
    "def visualize_tsne(X_text, y=None):\n",
    "    tsne = TSNE(n_components=2, random_state=42)\n",
    "    X_tsne = tsne.fit_transform(X_text)  # Reduce to 2D\n",
    "    \n",
    "    plt.figure(figsize=(8, 6))\n",
    "    if y is not None:  # If you have labels to color the points\n",
    "        plt.scatter(X_tsne[:, 0], X_tsne[:, 1], c=y, cmap='coolwarm', s=10)\n",
    "    else:\n",
    "        plt.scatter(X_tsne[:, 0], X_tsne[:, 1], s=10)\n",
    "    \n",
    "    plt.colorbar()\n",
    "    plt.title('t-SNE of DistilBERT Features')\n",
    "    plt.xlabel('t-SNE component 1')\n",
    "    plt.ylabel('t-SNE component 2')\n",
    "    plt.show()\n",
    "\n",
    "def extract_and_save_distilbert_features(data, batch_size=32, output_file='bert_features10000.h5'):\n",
    "    tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "    model = TFDistilBertModel.from_pretrained('distilbert-base-uncased')\n",
    "    \n",
    "    with h5py.File(output_file, 'w') as hf:\n",
    "        dset = hf.create_dataset(\"features\", (len(data), 768), dtype='float32')\n",
    "\n",
    "        for i, batch in enumerate(data_generator(data, batch_size)):\n",
    "            logging.info(f'Processing batch {i + 1}/{(len(data) + batch_size - 1) // batch_size}')\n",
    "            inputs = tokenizer(batch.tolist(), return_tensors='tf', padding=True, truncation=True, max_length=512)\n",
    "            \n",
    "            outputs = model(inputs)\n",
    "            batch_features = np.mean(outputs.last_hidden_state.numpy(), axis=1)\n",
    "            \n",
    "            dset[i * batch_size: i * batch_size + len(batch)] = batch_features\n",
    "            \n",
    "            gc.collect()\n",
    "    \n",
    "    logging.info('Feature extraction and saving completed')\n",
    "\n",
    "def load_bert_features(filename):\n",
    "    with h5py.File(filename, 'r') as hf:\n",
    "        features = hf['features'][:]\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'def plot_feature_importance(model, feature_names, top_n=20):\\n    importance = model.feature_importances_\\n    indices = np.argsort(importance)[-top_n:]\\n\\n    # Debugging outputs\\n    logging.info(f\"Feature importance values: {importance}\")\\n    logging.info(f\"Top {top_n} feature indices: {indices}\")\\n    logging.info(f\"Top {top_n} feature names: {[feature_names[i] for i in indices]}\")\\n    \\n    plt.figure(figsize=(10, 6))\\n    plt.title(\\'Feature Importance\\')\\n    plt.barh(range(len(indices)), importance[indices], color=\\'b\\', align=\\'center\\')\\n    plt.yticks(range(len(indices)), [feature_names[i] for i in indices])\\n    plt.xlabel(\\'Relative Importance\\')\\n    plt.show()'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#PLOT\n",
    "\n",
    "'''def plot_feature_importance(model, feature_names, top_n=20):\n",
    "    importance = model.feature_importances_\n",
    "    indices = np.argsort(importance)[-top_n:]\n",
    "\n",
    "    # Debugging outputs\n",
    "    logging.info(f\"Feature importance values: {importance}\")\n",
    "    logging.info(f\"Top {top_n} feature indices: {indices}\")\n",
    "    logging.info(f\"Top {top_n} feature names: {[feature_names[i] for i in indices]}\")\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.title('Feature Importance')\n",
    "    plt.barh(range(len(indices)), importance[indices], color='b', align='center')\n",
    "    plt.yticks(range(len(indices)), [feature_names[i] for i in indices])\n",
    "    plt.xlabel('Relative Importance')\n",
    "    plt.show()'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'def train_and_evaluate(X, y):\\n    # Split the data into training and test sets\\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\\n    \\n    # Init the classifier\\n    classifier = KNeighborsClassifier(n_neighbors=5)  # You can adjust n_neighbors as needed\\n    \\n    # Train the classifier\\n    classifier.fit(X_train, y_train)\\n    \\n    # Predict on the test \\n    y_pred = classifier.predict(X_test)\\n    \\n    # Print the metrics\\n    accuracy = accuracy_score(y_test, y_pred)\\n    f1 = f1_score(y_test, y_pred)\\n    precision = precision_score(y_test, y_pred)\\n    recall = recall_score(y_test, y_pred)\\n    roc_auc = roc_auc_score(y_test, y_pred)\\n    \\n    print(\"Accuracy:\", accuracy)\\n    print(\"F1 Score:\", f1)\\n    print(\"Precision:\", precision)\\n    print(\"Recall:\", recall)\\n    print(\"ROC AUC:\", roc_auc)\\n    print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))\\n\\n    #Confusion Matrix\\n    cm = confusion_matrix(y_test, y_pred)\\n    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=classifier.classes_)\\n    disp.plot(cmap=plt.cm.Blues)\\n    plt.title(\"Confusion Matrix\")\\n    plt.show()\\n\\n\\n    return classifier\\n'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "# Vectorize text data\n",
    "def vectorize_text(data):\n",
    "    tfidf_vectorizer = TfidfVectorizer(max_features=10000, stop_words='english')\n",
    "    X = tfidf_vectorizer.fit_transform(data)\n",
    "    return X, tfidf_vectorizer\n",
    "\n",
    "def normalize_features(df, feature_columns):\n",
    "    scaler = MinMaxScaler()\n",
    "    df[feature_columns] = scaler.fit_transform(df[feature_columns])\n",
    "    return df, scaler\n",
    "\n",
    "\n",
    "def combine_features(X_text, df, feature_columns):\n",
    "    X_numerical = df[feature_columns].values\n",
    "    \n",
    "    if X_text.shape[0] != X_numerical.shape[0]:\n",
    "        raise ValueError(f\"Mismatch in number of samples: X_text has {X_text.shape[0]} samples, while X_numerical has {X_numerical.shape[0]} samples.\")\n",
    "    \n",
    "    X = np.hstack((X_text, X_numerical))\n",
    "    return X\n",
    "\n",
    "\n",
    "#ANOTHER METHOD\n",
    "#K-Fold cross validation\n",
    "'''def train_and_evaluate(X, y):\n",
    "    classifier = xgb.XGBClassifier(use_label_encoder=False, eval_metric='logloss', reg_alpha=5, reg_lambda=5, max_depth=4, n_estimators=100)\n",
    "    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    accuracies = []\n",
    "    f1_scores = []\n",
    "    roc_aucs = []\n",
    "    precision_scores = []\n",
    "    recall_scores = []\n",
    "    roc_aucs = []\n",
    "\n",
    "    for train_index, test_index in skf.split(X, y):\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "        \n",
    "        classifier.fit(X_train, y_train)\n",
    "        y_pred = classifier.predict(X_test)\n",
    "        \n",
    "        accuracies.append(accuracy_score(y_test, y_pred))\n",
    "        f1_scores.append(f1_score(y_test, y_pred))\n",
    "        #precision_scores.append(precision_scores(y_test, y_pred))\n",
    "        roc_aucs.append(roc_auc_score(y_test, y_pred))\n",
    "        recall_scores.append(recall_score(y_test, y_pred))\n",
    "        roc_aucs.append(roc_auc_score(y_test, y_pred))\n",
    "        \n",
    "        print(\"Fold Results:\")\n",
    "        print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "        print(\"F1 Score:\", f1_score(y_test, y_pred))\n",
    "        print(\"Precision:\", precision_score(y_test, y_pred))\n",
    "        print(\"Recall:\", recall_score(y_test, y_pred))\n",
    "        print(\"ROC AUC:\", roc_auc_score(y_test, y_pred))\n",
    "        print(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n",
    "\n",
    "    print(\"Average Accuracy:\", np.mean(accuracies))\n",
    "    print(\"Average F1 Score:\", np.mean(f1_scores))\n",
    "    print(\"Average Precision:\", np.mean(precision_scores))\n",
    "    print(\"Average Recall:\", np.mean(recall_scores))\n",
    "    print(\"Average ROC AUC:\", np.mean(roc_aucs))\n",
    "\n",
    "    return classifier'''\n",
    "#K-fold without cross validation XGBOOST\n",
    "def train_and_evaluate(X, y):\n",
    "    # Split the data into training and test sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "    \n",
    "    # Init the classifier\n",
    "    classifier = xgb.XGBClassifier(use_label_encoder=False, eval_metric='logloss', reg_alpha=5, reg_lambda=5, max_depth=4, n_estimators=100)\n",
    "    \n",
    "    # Train the classifier\n",
    "    classifier.fit(X_train, y_train)\n",
    "    \n",
    "    # Predict on the test \n",
    "    y_pred = classifier.predict(X_test)\n",
    "    \n",
    "    # Print the metrics\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred)\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "    roc_auc = roc_auc_score(y_test, y_pred)\n",
    "    \n",
    "    print(\"Accuracy:\", accuracy)\n",
    "    print(\"F1 Score:\", f1)\n",
    "    print(\"Precision:\", precision)\n",
    "    print(\"Recall:\", recall)\n",
    "    print(\"ROC AUC:\", roc_auc)\n",
    "    print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))\n",
    "\n",
    "    #Confusion Matrix\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=classifier.classes_)\n",
    "    disp.plot(cmap=plt.cm.Blues)\n",
    "    plt.title(\"Confusion Matrix\")\n",
    "    plt.show()\n",
    "\n",
    "    return classifier\n",
    "\n",
    "\n",
    "#80/20\n",
    "'''def train_and_evaluate(X, y):\n",
    "    # Split the data into training and testing sets (80/20 split)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "    # Initialize the classifier\n",
    "    classifier = xgb.XGBClassifier(use_label_encoder=False, eval_metric='logloss', reg_alpha=5, reg_lambda=5, max_depth=4, n_estimators=100)\n",
    "    \n",
    "    # Train the classifier on the training set\n",
    "    classifier.fit(X_train, y_train)\n",
    "\n",
    "    # Predict on the test set\n",
    "    y_pred = classifier.predict(X_test)\n",
    "    \n",
    "    # Evaluate the classifier\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred)\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "    roc_auc = roc_auc_score(y_test, y_pred)\n",
    "    \n",
    "    print(\"Results on Test Set:\")\n",
    "    print(\"Accuracy:\", accuracy)\n",
    "    print(\"F1 Score:\", f1)\n",
    "    print(\"Precision:\", precision)\n",
    "    print(\"Recall:\", recall)\n",
    "    print(\"ROC AUC:\", roc_auc)\n",
    "    print(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n",
    "\n",
    "    return classifier'''\n",
    "\n",
    "\n",
    "# SVM\n",
    "'''def train_and_evaluate(X, y):\n",
    "    # Split the data into training and test sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "    \n",
    "    # Init the classifier\n",
    "    classifier = SVC(kernel='linear', probability=True)\n",
    "    \n",
    "    # Train the classifier\n",
    "    classifier.fit(X_train, y_train)\n",
    "    \n",
    "    # Predict on the test \n",
    "    y_pred = classifier.predict(X_test)\n",
    "    \n",
    "    # Print the metrics\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred)\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "    roc_auc = roc_auc_score(y_test, y_pred)\n",
    "    \n",
    "    print(\"Accuracy:\", accuracy)\n",
    "    print(\"F1 Score:\", f1)\n",
    "    print(\"Precision:\", precision)\n",
    "    print(\"Recall:\", recall)\n",
    "    print(\"ROC AUC:\", roc_auc)\n",
    "    print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))\n",
    "\n",
    "    return classifier'''\n",
    "\n",
    "\n",
    "#NAIVE BAYES\n",
    "'''def train_and_evaluate(X, y):\n",
    "    # Split the data into training and test sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "    \n",
    "    # Init the classifier\n",
    "    classifier = GaussianNB()\n",
    "    \n",
    "    # Train the classifier\n",
    "    classifier.fit(X_train, y_train)\n",
    "    \n",
    "    # Predict on the test \n",
    "    y_pred = classifier.predict(X_test)\n",
    "    \n",
    "    # Print the metrics\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred)\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "    roc_auc = roc_auc_score(y_test, y_pred)\n",
    "    \n",
    "    print(\"Accuracy:\", accuracy)\n",
    "    print(\"F1 Score:\", f1)\n",
    "    print(\"Precision:\", precision)\n",
    "    print(\"Recall:\", recall)\n",
    "    print(\"ROC AUC:\", roc_auc)\n",
    "    print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))\n",
    "\n",
    "    return classifier\n",
    "'''\n",
    "#KNN\n",
    "'''def train_and_evaluate(X, y):\n",
    "    # Split the data into training and test sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "    \n",
    "    # Init the classifier\n",
    "    classifier = KNeighborsClassifier(n_neighbors=5)  # You can adjust n_neighbors as needed\n",
    "    \n",
    "    # Train the classifier\n",
    "    classifier.fit(X_train, y_train)\n",
    "    \n",
    "    # Predict on the test \n",
    "    y_pred = classifier.predict(X_test)\n",
    "    \n",
    "    # Print the metrics\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred)\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "    roc_auc = roc_auc_score(y_test, y_pred)\n",
    "    \n",
    "    print(\"Accuracy:\", accuracy)\n",
    "    print(\"F1 Score:\", f1)\n",
    "    print(\"Precision:\", precision)\n",
    "    print(\"Recall:\", recall)\n",
    "    print(\"ROC AUC:\", roc_auc)\n",
    "    print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))\n",
    "\n",
    "    #Confusion Matrix\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=classifier.classes_)\n",
    "    disp.plot(cmap=plt.cm.Blues)\n",
    "    plt.title(\"Confusion Matrix\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "    return classifier\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class distribution:\n",
      "1    50000\n",
      "0    50000\n",
      "Name: label, dtype: int64\n",
      "Accuracy: 0.87285\n",
      "F1 Score: 0.8692881007453097\n",
      "Precision: 0.8943416181914331\n",
      "Recall: 0.8456\n",
      "ROC AUC: 0.8728500000000001\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.90      0.88     10000\n",
      "           1       0.89      0.85      0.87     10000\n",
      "\n",
      "    accuracy                           0.87     20000\n",
      "   macro avg       0.87      0.87      0.87     20000\n",
      "weighted avg       0.87      0.87      0.87     20000\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/ismayilzadamaharram/Desktop/Ameteur_Experience/Bachelor_Work/Notebooks/FreshStrartTraining1Hybrid.ipynb Cell 8'\u001b[0m in \u001b[0;36m<cell line: 57>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ismayilzadamaharram/Desktop/Ameteur_Experience/Bachelor_Work/Notebooks/FreshStrartTraining1Hybrid.ipynb#ch0000007?line=51'>52</a>\u001b[0m y \u001b[39m=\u001b[39m combined_df[\u001b[39m'\u001b[39m\u001b[39mlabel\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mvalues\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ismayilzadamaharram/Desktop/Ameteur_Experience/Bachelor_Work/Notebooks/FreshStrartTraining1Hybrid.ipynb#ch0000007?line=54'>55</a>\u001b[0m classifier \u001b[39m=\u001b[39m train_and_evaluate(X, y)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/ismayilzadamaharram/Desktop/Ameteur_Experience/Bachelor_Work/Notebooks/FreshStrartTraining1Hybrid.ipynb#ch0000007?line=56'>57</a>\u001b[0m visualize_tsne(X_text, y)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ismayilzadamaharram/Desktop/Ameteur_Experience/Bachelor_Work/Notebooks/FreshStrartTraining1Hybrid.ipynb#ch0000007?line=58'>59</a>\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mxgb_model.pkl\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mwb\u001b[39m\u001b[39m'\u001b[39m) \u001b[39mas\u001b[39;00m model_file:\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ismayilzadamaharram/Desktop/Ameteur_Experience/Bachelor_Work/Notebooks/FreshStrartTraining1Hybrid.ipynb#ch0000007?line=59'>60</a>\u001b[0m     pickle\u001b[39m.\u001b[39mdump(classifier, model_file)\n",
      "\u001b[1;32m/Users/ismayilzadamaharram/Desktop/Ameteur_Experience/Bachelor_Work/Notebooks/FreshStrartTraining1Hybrid.ipynb Cell 5'\u001b[0m in \u001b[0;36mvisualize_tsne\u001b[0;34m(X_text, y)\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/ismayilzadamaharram/Desktop/Ameteur_Experience/Bachelor_Work/Notebooks/FreshStrartTraining1Hybrid.ipynb#ch0000004?line=6'>7</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mvisualize_tsne\u001b[39m(X_text, y\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/ismayilzadamaharram/Desktop/Ameteur_Experience/Bachelor_Work/Notebooks/FreshStrartTraining1Hybrid.ipynb#ch0000004?line=7'>8</a>\u001b[0m     tsne \u001b[39m=\u001b[39m TSNE(n_components\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m, random_state\u001b[39m=\u001b[39m\u001b[39m42\u001b[39m)\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/ismayilzadamaharram/Desktop/Ameteur_Experience/Bachelor_Work/Notebooks/FreshStrartTraining1Hybrid.ipynb#ch0000004?line=8'>9</a>\u001b[0m     X_tsne \u001b[39m=\u001b[39m tsne\u001b[39m.\u001b[39;49mfit_transform(X_text)  \u001b[39m# Reduce to 2D\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ismayilzadamaharram/Desktop/Ameteur_Experience/Bachelor_Work/Notebooks/FreshStrartTraining1Hybrid.ipynb#ch0000004?line=10'>11</a>\u001b[0m     plt\u001b[39m.\u001b[39mfigure(figsize\u001b[39m=\u001b[39m(\u001b[39m8\u001b[39m, \u001b[39m6\u001b[39m))\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ismayilzadamaharram/Desktop/Ameteur_Experience/Bachelor_Work/Notebooks/FreshStrartTraining1Hybrid.ipynb#ch0000004?line=11'>12</a>\u001b[0m     \u001b[39mif\u001b[39;00m y \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:  \u001b[39m# If you have labels to color the points\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/_set_output.py:313\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[0;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[1;32m    311\u001b[0m \u001b[39m@wraps\u001b[39m(f)\n\u001b[1;32m    312\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwrapped\u001b[39m(\u001b[39mself\u001b[39m, X, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m--> 313\u001b[0m     data_to_wrap \u001b[39m=\u001b[39m f(\u001b[39mself\u001b[39;49m, X, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    314\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(data_to_wrap, \u001b[39mtuple\u001b[39m):\n\u001b[1;32m    315\u001b[0m         \u001b[39m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[1;32m    316\u001b[0m         return_tuple \u001b[39m=\u001b[39m (\n\u001b[1;32m    317\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[39m0\u001b[39m], X, \u001b[39mself\u001b[39m),\n\u001b[1;32m    318\u001b[0m             \u001b[39m*\u001b[39mdata_to_wrap[\u001b[39m1\u001b[39m:],\n\u001b[1;32m    319\u001b[0m         )\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/sklearn/base.py:1473\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1466\u001b[0m     estimator\u001b[39m.\u001b[39m_validate_params()\n\u001b[1;32m   1468\u001b[0m \u001b[39mwith\u001b[39;00m config_context(\n\u001b[1;32m   1469\u001b[0m     skip_parameter_validation\u001b[39m=\u001b[39m(\n\u001b[1;32m   1470\u001b[0m         prefer_skip_nested_validation \u001b[39mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1471\u001b[0m     )\n\u001b[1;32m   1472\u001b[0m ):\n\u001b[0;32m-> 1473\u001b[0m     \u001b[39mreturn\u001b[39;00m fit_method(estimator, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/sklearn/manifold/_t_sne.py:1176\u001b[0m, in \u001b[0;36mTSNE.fit_transform\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m   1173\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_max_iter \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmax_iter\n\u001b[1;32m   1175\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_params_vs_input(X)\n\u001b[0;32m-> 1176\u001b[0m embedding \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fit(X)\n\u001b[1;32m   1177\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membedding_ \u001b[39m=\u001b[39m embedding\n\u001b[1;32m   1178\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membedding_\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/sklearn/manifold/_t_sne.py:1044\u001b[0m, in \u001b[0;36mTSNE._fit\u001b[0;34m(self, X, skip_num_points)\u001b[0m\n\u001b[1;32m   1038\u001b[0m \u001b[39m# Degrees of freedom of the Student's t-distribution. The suggestion\u001b[39;00m\n\u001b[1;32m   1039\u001b[0m \u001b[39m# degrees_of_freedom = n_components - 1 comes from\u001b[39;00m\n\u001b[1;32m   1040\u001b[0m \u001b[39m# \"Learning a Parametric Embedding by Preserving Local Structure\"\u001b[39;00m\n\u001b[1;32m   1041\u001b[0m \u001b[39m# Laurens van der Maaten, 2009.\u001b[39;00m\n\u001b[1;32m   1042\u001b[0m degrees_of_freedom \u001b[39m=\u001b[39m \u001b[39mmax\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_components \u001b[39m-\u001b[39m \u001b[39m1\u001b[39m, \u001b[39m1\u001b[39m)\n\u001b[0;32m-> 1044\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_tsne(\n\u001b[1;32m   1045\u001b[0m     P,\n\u001b[1;32m   1046\u001b[0m     degrees_of_freedom,\n\u001b[1;32m   1047\u001b[0m     n_samples,\n\u001b[1;32m   1048\u001b[0m     X_embedded\u001b[39m=\u001b[39;49mX_embedded,\n\u001b[1;32m   1049\u001b[0m     neighbors\u001b[39m=\u001b[39;49mneighbors_nn,\n\u001b[1;32m   1050\u001b[0m     skip_num_points\u001b[39m=\u001b[39;49mskip_num_points,\n\u001b[1;32m   1051\u001b[0m )\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/sklearn/manifold/_t_sne.py:1112\u001b[0m, in \u001b[0;36mTSNE._tsne\u001b[0;34m(self, P, degrees_of_freedom, n_samples, X_embedded, neighbors, skip_num_points)\u001b[0m\n\u001b[1;32m   1110\u001b[0m     opt_args[\u001b[39m\"\u001b[39m\u001b[39mmomentum\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39m0.8\u001b[39m\n\u001b[1;32m   1111\u001b[0m     opt_args[\u001b[39m\"\u001b[39m\u001b[39mn_iter_without_progress\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_iter_without_progress\n\u001b[0;32m-> 1112\u001b[0m     params, kl_divergence, it \u001b[39m=\u001b[39m _gradient_descent(obj_func, params, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mopt_args)\n\u001b[1;32m   1114\u001b[0m \u001b[39m# Save the final number of iterations\u001b[39;00m\n\u001b[1;32m   1115\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_iter_ \u001b[39m=\u001b[39m it\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/sklearn/manifold/_t_sne.py:403\u001b[0m, in \u001b[0;36m_gradient_descent\u001b[0;34m(objective, p0, it, max_iter, n_iter_check, n_iter_without_progress, momentum, learning_rate, min_gain, min_grad_norm, verbose, args, kwargs)\u001b[0m\n\u001b[1;32m    400\u001b[0m \u001b[39m# only compute the error when needed\u001b[39;00m\n\u001b[1;32m    401\u001b[0m kwargs[\u001b[39m\"\u001b[39m\u001b[39mcompute_error\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m check_convergence \u001b[39mor\u001b[39;00m i \u001b[39m==\u001b[39m max_iter \u001b[39m-\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m--> 403\u001b[0m error, grad \u001b[39m=\u001b[39m objective(p, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    405\u001b[0m inc \u001b[39m=\u001b[39m update \u001b[39m*\u001b[39m grad \u001b[39m<\u001b[39m \u001b[39m0.0\u001b[39m\n\u001b[1;32m    406\u001b[0m dec \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39minvert(inc)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/sklearn/manifold/_t_sne.py:280\u001b[0m, in \u001b[0;36m_kl_divergence_bh\u001b[0;34m(params, P, degrees_of_freedom, n_samples, n_components, angle, skip_num_points, verbose, compute_error, num_threads)\u001b[0m\n\u001b[1;32m    277\u001b[0m X_embedded \u001b[39m=\u001b[39m params\u001b[39m.\u001b[39mreshape(n_samples, n_components)\n\u001b[1;32m    279\u001b[0m val_P \u001b[39m=\u001b[39m P\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mastype(np\u001b[39m.\u001b[39mfloat32, copy\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[0;32m--> 280\u001b[0m neighbors \u001b[39m=\u001b[39m P\u001b[39m.\u001b[39;49mindices\u001b[39m.\u001b[39;49mastype(np\u001b[39m.\u001b[39;49mint64, copy\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n\u001b[1;32m    281\u001b[0m indptr \u001b[39m=\u001b[39m P\u001b[39m.\u001b[39mindptr\u001b[39m.\u001b[39mastype(np\u001b[39m.\u001b[39mint64, copy\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[1;32m    283\u001b[0m grad \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mzeros(X_embedded\u001b[39m.\u001b[39mshape, dtype\u001b[39m=\u001b[39mnp\u001b[39m.\u001b[39mfloat32)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Execution block\n",
    "merged_file = \"../Merged_troll_non_troll/merged_dataset100000.csv\"\n",
    "\n",
    "\n",
    "# Load troll and non-troll datasets\n",
    "combined_df = load_merged_data(merged_file)\n",
    "\n",
    "\n",
    "\n",
    "# Investigate class distribution\n",
    "print(\"Class distribution:\")\n",
    "print(combined_df['label'].value_counts())\n",
    "\n",
    "# Define text and numerical feature columns\n",
    "text_data = combined_df['processed_content']\n",
    "numerical_features = ['char_count', 'word_count', 'sentence_count', 'avg_word_length',\n",
    "                      'sentiment_neg', 'sentiment_neu', 'sentiment_pos', 'sentiment_compound',\n",
    "                      'lexical_diversity', 'subjectivity']\n",
    "assert len(text_data) == len(combined_df), \"Length of text data and combined DataFrame do not match\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Vectorize text data\n",
    "#X_text, vectorizer = vectorize_text(text_data)\n",
    "\n",
    "\n",
    "\n",
    "#X_text = extract_distilbert_features(text_data, batch_size=16)\n",
    "\n",
    "# Extract and save DistilBERT features\n",
    "#extract_and_save_distilbert_features(text_data, batch_size=32, output_file='bert_features100000.h5')\n",
    "\n",
    "# Load DistilBERT features from disk\n",
    "X_text = load_bert_features('bert_features100000.h5')\n",
    "\n",
    "# Handle NaN values if present by filling with zeros\n",
    "combined_df[numerical_features] = combined_df[numerical_features].fillna(0)\n",
    "\n",
    "\n",
    "#Normalize num features\n",
    "combined_df, scaler = normalize_features(combined_df, numerical_features)\n",
    "\n",
    "#Combine text and num features\n",
    "X = combine_features(X_text, combined_df, numerical_features)\n",
    "y = combined_df['label'].values\n",
    "\n",
    "\n",
    "classifier = train_and_evaluate(X, y)\n",
    "\n",
    "visualize_tsne(X_text, y)\n",
    "\n",
    "with open('xgb_model.pkl', 'wb') as model_file:\n",
    "    pickle.dump(classifier, model_file)\n",
    "with open('scaler.pkl', 'wb') as scaler_file:\n",
    "    pickle.dump(scaler, scaler_file)\n",
    "\n",
    "\n",
    "# Since DISTILBERT features are not interpretable in the same way as other features, we skip this step for BERT features\n",
    "#feature_names = [f'bert_feature_{i}' for i in range(X_text.shape[1])] + numerical_features\n",
    "#plot_feature_importance(classifier, feature_names)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "72229e2cb4a4d3f9a66c0f3bbf8721a4f899e8fc91e357b565a53efd017c27d6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
