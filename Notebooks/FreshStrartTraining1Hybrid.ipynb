{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/ismayilzadamaharram/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/ismayilzadamaharram/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     /Users/ismayilzadamaharram/nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import h5py\n",
    "import pickle\n",
    "\n",
    "\n",
    "\n",
    "import gc\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score, classification_report, precision_score, recall_score, confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.utils import resample\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from textblob import TextBlob\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from transformers import DistilBertTokenizer, DistilBertModel, TFDistilBertModel\n",
    "#from transformers import TFAutoModel, AutoTokenizer\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#from gensim.models import Word2Vec\n",
    "\n",
    "import joblib\n",
    "import xgboost as xgb\n",
    "import nltk\n",
    "import seaborn as sns\n",
    "#import shap\n",
    "import logging\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "\n",
    "from textstat import textstat\n",
    "from multiprocessing import Pool\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"]=\"TRUE\"\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('vader_lexicon')\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "###PREPROCESSING PART\n",
    "\n",
    "SYMBOLS_TO_ISOLATE = '.,?!-;*\"â€¦:â€”()%#$&_/@ï¼¼ãƒ»Ï‰+=â€â€œ[]^â€“>\\\\Â°<~â€¢â‰ â„¢ÊŠÉ’âˆžÂ§{}Â·Ï„Î±â¤â˜ºÉ¡|Â¢â†’Ì¶`â¥â”â”£â”«â”—ï¼¯â–ºâ˜…Â©â€•Éªâœ”Â®\\x96\\x92â—Â£â™¥âž¤Â´Â¹â˜•â‰ˆÃ·â™¡â—â•‘â–¬â€²É”Ëâ‚¬Û©Ûžâ€ Î¼âœ’âž¥â•â˜†ËŒâ—„Â½Ê»Ï€Î´Î·Î»ÏƒÎµÏÎ½Êƒâœ¬ï¼³ï¼µï¼°ï¼¥ï¼²ï¼©ï¼´â˜»Â±â™ÂµÂºÂ¾âœ“â—¾ØŸï¼Žâ¬…â„…Â»Ð’Ð°Ð²â£â‹…Â¿Â¬â™«ï¼£ï¼­Î²â–ˆâ–“â–’â–‘â‡’â­â€ºÂ¡â‚‚â‚ƒâ§â–°â–”â—žâ–€â–‚â–ƒâ–„â–…â–†â–‡â†™Î³Ì„â€³â˜¹âž¡Â«Ï†â…“â€žâœ‹ï¼šÂ¥Ì²Ì…Ìâˆ™â€›â—‡âœâ–·â“â—Â¶ËšË™ï¼‰ÑÐ¸Ê¿âœ¨ã€‚É‘\\x80â—•ï¼ï¼…Â¯âˆ’ï¬‚ï¬â‚Â²ÊŒÂ¼â´â„â‚„âŒ â™­âœ˜â•ªâ–¶â˜­âœ­â™ªâ˜”â˜ â™‚â˜ƒâ˜ŽâœˆâœŒâœ°â†â˜™â—‹â€£âš“å¹´âˆŽâ„’â–ªâ–™â˜â…›ï½ƒï½ï½“Ç€â„®Â¸ï½—â€šâˆ¼â€–â„³â„â†â˜¼â‹†Ê’âŠ‚ã€â…”Â¨Í¡à¹âš¾âš½Î¦Ã—Î¸ï¿¦ï¼Ÿï¼ˆâ„ƒâ©â˜®âš æœˆâœŠâŒâ­•â–¸â– â‡Œâ˜â˜‘âš¡â˜„Ç«â•­âˆ©â•®ï¼Œä¾‹ï¼žÊ•ÉÌ£Î”â‚€âœžâ”ˆâ•±â•²â–â–•â”ƒâ•°â–Šâ–‹â•¯â”³â”Šâ‰¥â˜’â†‘â˜É¹âœ…â˜›â™©â˜žï¼¡ï¼ªï¼¢â—”â—¡â†“â™€â¬†Ì±â„\\x91â €Ë¤â•šâ†ºâ‡¤âˆâœ¾â—¦â™¬Â³ã®ï½œï¼âˆµâˆ´âˆšÎ©Â¤â˜œâ–²â†³â–«â€¿â¬‡âœ§ï½ï½–ï½ï¼ï¼’ï¼ï¼˜ï¼‡â€°â‰¤âˆ•Ë†âšœâ˜'\n",
    "SYMBOLS_TO_REMOVE = '\\n\\r\\xa0\\ue014\\t\\uf818\\uf04a\\xad\\uf0e0\\u200b\\u200eØ¹Ø¯ÙˆÙŠÙ‡ØµÙ‚Ø£Ù†Ø§Ø®Ù„Ù‰Ø¨Ù…ØºØ±Ð•\\u202a\\u202cðŸ»á´ÊÊ€á´‡É´á´…á´á´€á´‹Êœá´œÊŸá´›á´„á´˜Ê™Ò“á´Šá´¡É¢×©×œ×•××‘×™â€¼\\x81ã‚¨ãƒ³ã‚¸æ•…éšœ\\u2009á´µÍž\\u200f××¢×›×—à®œá´ â€\\x7fá¼á½¶Î®Î¹á½²Îºá¼€Î¯á¿ƒá¼´Î¾ï¼¨\\ufeff\\u2028\\u3000ØªØ­ÙƒØ³Ø©ðŸ‘®ðŸ’™ÙØ²Ø·\\u2008ðŸ¾\\x08â€‘åœ°ç„è°·ÑƒÐ»ÐºÐ½ÐŸÐ¾ÐÐæ­Œèˆžä¼Ž×”Ð¼Ï…Ñ‚Ñ•â¤µ\\u200aÑÐ¿Ñ€Ð´\\x95\\u2002\\u2004Ë¢áµ’Ê³Ê¸á´¼á´·á´ºÊ·áµ—Ê°áµ‰áµ˜\\x13\\ue602Î¬Î¿ÏŒÏ‚Î­á½¸×ª×ž×“×£× ×¨×š×¦×˜\\uf0b7\\uf04c\\x9f\\x10æˆéƒ½\\u202dðŸ’¤ðŸ‡\\ue613å°åœŸè±†ðŸ¡â”â‰\\u202fðŸ‘ ã€‹à¤•à¤°à¥à¤®à¤¾ðŸ‡¹ðŸ‡¼ðŸŒ¸è”¡è‹±æ–‡ðŸŒžðŸŽ²ãƒ¬ã‚¯ã‚µã‚¹å¤–å›½äººå…³ç³»Ð¡Ð±ðŸ’‹ðŸ’€ðŸŽ„ðŸ’œÑŒÑ‹Ð³Ñä¸æ˜¯\\x9c\\x9dðŸ—‘\\u2005ðŸ’ƒðŸ“£à¼¼ã¤à¼½á¸·Ð—Ð·â–±Ñ†ï¿¼å–æ¸©å“¥åŽè®®ä¼šä¸‹é™ä½ å¤±åŽ»æ‰€æœ‰çš„é’±åŠ æ‹¿å¤§åç¨Žéª—å­ðŸãƒ„ðŸŽ…\\x85ðŸºØ¢Ø¥Ø´Ø¡ðŸŽµðŸŒŽÍŸá¼”æ²¹åˆ«å…‹ðŸ¤¡Ð¹\\u2003ðŸš€ðŸ¤´Ê²ÑˆÑ‡Ð˜ÐžÐ Ð¤Ð”Ð¯ÐœÑŽÐ¶ðŸ–‘á½á½»Ïç‰¹æ®Šä½œæˆ¦ç¾¤Ñ‰ðŸ’¨åœ†æ˜Žå›­×§â„\\u200dð“’ð“²ð“¿ð“µì•ˆì˜í•˜ì„¸ìš”Ð–Ñ™ÐšÑ›ðŸ€ðŸ˜«ðŸ¤¤á¿¦æˆ‘å‡ºç”Ÿåœ¨äº†å¯ä»¥è¯´æ™®é€šè¯æ±‰è¯­å¥½æžðŸŽ¼ðŸ•ºðŸ¸ðŸ¥‚ðŸ—½ðŸŽ‡ðŸŽŠðŸ†˜ðŸ¤ ðŸ‘©ðŸ–’ðŸšªå¤©ä¸€å®¶âš²\\u2006âš­âš†â¬­â¬¯â–æ–°âœ€â•ŒðŸ‡«ðŸ‡·ðŸ‡©ðŸ‡ªðŸ‡®ðŸ‡¬ðŸ‡§ðŸ˜·ðŸ‡¨ðŸ‡¦Ð¥Ð¨ðŸŒ\\x1fæ€é¸¡ç»™çŒ´çœ‹Êð—ªð—µð—²ð—»ð˜†ð—¼ð˜‚ð—¿ð—®ð—¹ð—¶ð˜‡ð—¯ð˜ð—°ð˜€ð˜…ð—½ð˜„ð—±ðŸ“ºÏ–\\u2000Ò¯Õ½á´¦áŽ¥Ò»Íº\\u2007Õ°\\u2001É©ï½™ï½…àµ¦ï½ŒÆ½ï½ˆð“ð¡ðžð«ð®ððšðƒðœð©ð­ð¢ð¨ð§Æ„á´¨×Ÿá‘¯à»Î¤á§à¯¦Ð†á´‘Üð¬ð°ð²ð›ð¦ð¯ð‘ð™ð£ð‡ð‚ð˜ðŸŽÔœÐ¢á—žà±¦ã€”áŽ«ð³ð”ð±ðŸ”ðŸ“ð…ðŸ‹ï¬ƒðŸ’˜ðŸ’“Ñ‘ð˜¥ð˜¯ð˜¶ðŸ’ðŸŒ‹ðŸŒ„ðŸŒ…ð™¬ð™–ð™¨ð™¤ð™£ð™¡ð™®ð™˜ð™ ð™šð™™ð™œð™§ð™¥ð™©ð™ªð™—ð™žð™ð™›ðŸ‘ºðŸ·â„‹ð€ð¥ðªðŸš¶ð™¢á¼¹ðŸ¤˜Í¦ðŸ’¸Ø¬íŒ¨í‹°ï¼·ð™‡áµ»ðŸ‘‚ðŸ‘ƒÉœðŸŽ«\\uf0a7Ð‘Ð£Ñ–ðŸš¢ðŸš‚àª—à«àªœàª°àª¾àª¤à«€á¿†ðŸƒð“¬ð“»ð“´ð“®ð“½ð“¼â˜˜ï´¾Ì¯ï´¿â‚½\\ue807ð‘»ð’†ð’ð’•ð’‰ð’“ð’–ð’‚ð’ð’…ð’”ð’Žð’—ð’ŠðŸ‘½ðŸ˜™\\u200cÐ›â€’ðŸŽ¾ðŸ‘¹âŽŒðŸ’â›¸å…¬å¯“å…»å® ç‰©å—ðŸ„ðŸ€ðŸš‘ðŸ¤·æ“ç¾Žð’‘ð’šð’ð‘´ðŸ¤™ðŸ’æ¬¢è¿Žæ¥åˆ°é˜¿æ‹‰æ–¯×¡×¤ð™«ðŸˆð’Œð™Šð™­ð™†ð™‹ð™ð˜¼ð™…ï·»ðŸ¦„å·¨æ”¶èµ¢å¾—ç™½é¬¼æ„¤æ€’è¦ä¹°é¢áº½ðŸš—ðŸ³ðŸðŸðŸ–ðŸ‘ðŸ•ð’„ðŸ—ð ð™„ð™ƒðŸ‘‡é”Ÿæ–¤æ‹·ð—¢ðŸ³ðŸ±ðŸ¬â¦ãƒžãƒ«ãƒãƒ‹ãƒãƒ­æ ªå¼ç¤¾â›·í•œêµ­ì–´ã„¸ã…“ë‹ˆÍœÊ–ð˜¿ð™”â‚µð’©â„¯ð’¾ð“ð’¶ð“‰ð“‡ð“Šð“ƒð“ˆð“…â„´ð’»ð’½ð“€ð“Œð’¸ð“Žð™Î¶ð™Ÿð˜ƒð—ºðŸ®ðŸ­ðŸ¯ðŸ²ðŸ‘‹ðŸ¦Šå¤šä¼¦ðŸ½ðŸŽ»ðŸŽ¹â›“ðŸ¹ðŸ·ðŸ¦†ä¸ºå’Œä¸­å‹è°Šç¥è´ºä¸Žå…¶æƒ³è±¡å¯¹æ³•å¦‚ç›´æŽ¥é—®ç”¨è‡ªå·±çŒœæœ¬ä¼ æ•™å£«æ²¡ç§¯å”¯è®¤è¯†åŸºç£å¾’æ›¾ç»è®©ç›¸ä¿¡è€¶ç¨£å¤æ´»æ­»æ€ªä»–ä½†å½“ä»¬èŠäº›æ”¿æ²»é¢˜æ—¶å€™æˆ˜èƒœå› åœ£æŠŠå…¨å ‚ç»“å©šå­©ææƒ§ä¸”æ —è°“è¿™æ ·è¿˜â™¾ðŸŽ¸ðŸ¤•ðŸ¤’â›‘ðŸŽæ‰¹åˆ¤æ£€è®¨ðŸðŸ¦ðŸ™‹ðŸ˜¶ì¥ìŠ¤íƒ±íŠ¸ë¤¼ë„ì„ìœ ê°€ê²©ì¸ìƒì´ê²½ì œí™©ì„ë µê²Œë§Œë“¤ì§€ì•Šë¡ìž˜ê´€ë¦¬í•´ì•¼í•©ë‹¤ìºë‚˜ì—ì„œëŒ€ë§ˆì´ˆì™€í™”ì•½ê¸ˆì˜í’ˆëŸ°ì„±ë¶„ê°ˆë•ŒëŠ”ë°˜ë“œì‹œí—ˆëœì‚¬ìš©ðŸ”«ðŸ‘å‡¸á½°ðŸ’²ðŸ—¯ð™ˆá¼Œð’‡ð’ˆð’˜ð’ƒð‘¬ð‘¶ð•¾ð–™ð–—ð–†ð–Žð–Œð–ð–•ð–Šð–”ð–‘ð–‰ð–“ð–ð–œð–žð–šð–‡ð•¿ð–˜ð–„ð–›ð–’ð–‹ð–‚ð•´ð–Ÿð–ˆð•¸ðŸ‘‘ðŸš¿ðŸ’¡çŸ¥å½¼ç™¾\\uf005ð™€ð’›ð‘²ð‘³ð‘¾ð’‹ðŸ’ðŸ˜¦ð™’ð˜¾ð˜½ðŸð˜©ð˜¨á½¼á¹‘ð‘±ð‘¹ð‘«ð‘µð‘ªðŸ‡°ðŸ‡µðŸ‘¾á“‡á’§á”­áƒá§á¦á‘³á¨á“ƒá“‚á‘²á¸á‘­á‘Žá“€á£ðŸ„ðŸŽˆðŸ”¨ðŸŽðŸ¤žðŸ¸ðŸ’ŸðŸŽ°ðŸŒðŸ›³ç‚¹å‡»æŸ¥ç‰ˆðŸ­ð‘¥ð‘¦ð‘§ï¼®ï¼§ðŸ‘£\\uf020ã£ðŸ‰Ñ„ðŸ’­ðŸŽ¥ÎžðŸ´ðŸ‘¨ðŸ¤³ðŸ¦\\x0bðŸ©ð‘¯ð’’ðŸðŸ‚ðŸ‘³ðŸ—ðŸ•‰ðŸ²Ú†ÛŒð‘®ð—•ð—´ðŸ’êœ¥â²£â²ðŸ‘â°é‰„ãƒªäº‹ä»¶Ñ—ðŸ’Šã€Œã€\\uf203\\uf09a\\uf222\\ue608\\uf202\\uf099\\uf469\\ue607\\uf410\\ue600ç‡»è£½ã‚·è™šå½å±ç†å±ˆÐ“ð‘©ð‘°ð’€ð‘ºðŸŒ¤ð—³ð—œð—™ð—¦ð—§ðŸŠá½ºá¼ˆá¼¡Ï‡á¿–Î›â¤ðŸ‡³ð’™ÏˆÕÕ´Õ¥Õ¼Õ¡ÕµÕ«Õ¶Ö€Ö‚Õ¤Õ±å†¬è‡³á½€ð’ðŸ”¹ðŸ¤šðŸŽð‘·ðŸ‚ðŸ’…ð˜¬ð˜±ð˜¸ð˜·ð˜ð˜­ð˜“ð˜–ð˜¹ð˜²ð˜«Ú©Î’ÏŽðŸ’¢ÎœÎŸÎÎ‘Î•ðŸ‡±â™²ðˆâ†´ðŸ’’âŠ˜È»ðŸš´ðŸ–•ðŸ–¤ðŸ¥˜ðŸ“ðŸ‘ˆâž•ðŸš«ðŸŽ¨ðŸŒ‘ðŸ»ðŽððŠð‘­ðŸ¤–ðŸŽŽðŸ˜¼ðŸ•·ï½‡ï½’ï½Žï½”ï½‰ï½„ï½•ï½†ï½‚ï½‹ðŸ°ðŸ‡´ðŸ‡­ðŸ‡»ðŸ‡²ð—žð—­ð—˜ð—¤ðŸ‘¼ðŸ“‰ðŸŸðŸ¦ðŸŒˆðŸ”­ã€ŠðŸŠðŸ\\uf10aáƒšÚ¡ðŸ¦\\U0001f92f\\U0001f92aðŸ¡ðŸ’³á¼±ðŸ™‡ð—¸ð—Ÿð— ð—·ðŸ¥œã•ã‚ˆã†ãªã‚‰ðŸ”¼'\n",
    "POSITIVE_EMOJI = 'ðŸ˜œðŸ˜ŽðŸ˜ðŸ’–ðŸ˜€ðŸ˜‚ðŸ˜„ðŸ˜‹ðŸ‘ðŸ˜ŠðŸ‘ðŸ˜ƒðŸ˜˜ðŸ‘ŒðŸ™‚ðŸ˜‰ðŸ˜ðŸŽ‰ðŸ˜…ðŸ‘»ðŸ™ƒðŸ˜†ðŸ¤—ðŸ¤“ðŸ˜ŒðŸ¤‘ðŸ˜›ðŸ¤£ðŸ˜ðŸ’ªðŸ˜—ðŸ¥°ðŸ˜‡ðŸ¤ ðŸ¤¡ðŸ¥³ðŸ¥´ðŸ¤©ðŸ˜ºðŸ˜¸ðŸ˜¹ðŸ˜»ðŸ˜½âœŒï¸ðŸ¤ŸðŸ¤˜'\n",
    "NEGATIVE_EMOJI = 'ðŸ˜¢ðŸ‘ŽðŸ˜±ðŸ˜³ðŸ˜§ðŸ™€ðŸ˜ðŸ˜•ðŸ˜®ðŸ˜–ðŸ˜ŸðŸ˜¡ðŸ˜ ðŸ˜¤ðŸ˜žðŸ˜­ðŸ˜¥ðŸ˜”ðŸ˜“ðŸ˜ªðŸ˜¨ðŸ˜©ðŸ™ðŸ˜µðŸ˜’ÍðŸ˜£ðŸ˜²ðŸ˜¯ðŸ¤¢ÙÙŽðŸ˜°ðŸ‘¿ðŸ‘¿ðŸ¤¥ðŸ˜¬ðŸ˜·ðŸ¤’ðŸ¤•ðŸ¤¯ðŸ¤¬ðŸ¥ºðŸ™€ðŸ˜¿ðŸ˜¾ðŸ–•ðŸ»ðŸ–•ðŸ¼'\n",
    "NEUTRAL_EMOJI = 'ðŸ¶ï¸ðŸ•ðŸµðŸ’µðŸ”¥ðŸ’¥ðŸšŒðŸŒŸðŸ’©ðŸ’¯â›½ðŸš„ðŸ¼ðŸš²ðŸ˜ˆðŸ™ðŸŽ¯ðŸŒ¹ðŸ’”ðŸ‘ŠðŸ™„â›ºðŸ¾ðŸ½ðŸŽ†ðŸ»âºðŸŒðŸ’žðŸš“ðŸ””ðŸ“šðŸ€ðŸ‘ðŸ½ðŸŽ¶ðŸŒºðŸ¤”ðŸ°ðŸ‡ðŸˆðŸ˜ºðŸŒðŸ”ðŸ®ðŸðŸ†ðŸ‘ðŸŒ®ðŸŒ¯ðŸ¤¦ðŸ™ˆðŸ˜´ðŸ†•ðŸ‘…ðŸ‘¥ðŸ‘„ðŸ”„ðŸ”¤ðŸ‘‰ðŸ‘¤ðŸ¤§ðŸ‘¶ðŸ‘²ðŸ”›ðŸŽ“ðŸ¿ðŸ‡ºðŸ‡¸ðŸŒ ðŸŸðŸ’«ðŸ’°ðŸš¬ðŸ’ŽðŸ±ðŸ™†ðŸ’•ð˜Šð˜¦ð˜³ð˜¢ð˜µð˜°ð˜¤ð˜ºð˜´ð˜ªð˜§ð˜®ð˜£ðŸ’—ðŸ’šðŸ¾ðŸ•ðŸ”—ðŸš½ðŸ†ðŸŽƒðŸ–ðŸ™…â›²ðŸ°ðŸ¤ðŸ‘†ðŸ™ŒðŸ’›ðŸ‘€ðŸ™ŠðŸ™‰'\n",
    "ISOLATE_DICT = {ord(c):' special symbol '.format(ord(c)) for c in SYMBOLS_TO_ISOLATE}\n",
    "REMOVE_DICT = {ord(c):'' for c in SYMBOLS_TO_REMOVE}\n",
    "NEUTRAL_EMOJI_DICT = {ord(c):' neutral emoji ' for c in NEUTRAL_EMOJI}\n",
    "POSITIVE_EMOJI_DICT = {ord(c):' positive emoji ' for c in POSITIVE_EMOJI}\n",
    "NEGATIVE_EMOJI_DICT = {ord(c):' negative emoji ' for c in NEGATIVE_EMOJI}\n",
    "CONTRACTION_MAPPING = {\"ain't\": \"is not\", \"aren't\": \"are not\",\"can't\": \"cannot\", \"'cause\": \"because\", \"could've\": \"could have\", \"couldn't\": \"could not\", \"didn't\": \"did not\",  \"doesn't\": \"does not\", \"don't\": \"do not\", \"hadn't\": \"had not\", \"hasn't\": \"has not\", \"haven't\": \"have not\", \"he'd\": \"he would\",\"he'll\": \"he will\", \"he's\": \"he is\", \"how'd\": \"how did\", \"how'd'y\": \"how do you\", \"how'll\": \"how will\", \"how's\": \"how is\",  \"I'd\": \"I would\", \"I'd've\": \"I would have\", \"I'll\": \"I will\", \"I'll've\": \"I will have\",\"I'm\": \"I am\", \"I've\": \"I have\", \"i'd\": \"i would\", \"i'd've\": \"i would have\", \"i'll\": \"i will\",  \"i'll've\": \"i will have\",\"i'm\": \"i am\", \"i've\": \"i have\", \"isn't\": \"is not\", \"it'd\": \"it would\", \"it'd've\": \"it would have\", \"it'll\": \"it will\", \"it'll've\": \"it will have\",\"it's\": \"it is\", \"let's\": \"let us\", \"ma'am\": \"madam\", \"mayn't\": \"may not\", \"might've\": \"might have\",\"mightn't\": \"might not\",\"mightn't've\": \"might not have\", \"must've\": \"must have\", \"mustn't\": \"must not\", \"mustn't've\": \"must not have\", \"needn't\": \"need not\", \"needn't've\": \"need not have\",\"o'clock\": \"of the clock\", \"oughtn't\": \"ought not\", \"oughtn't've\": \"ought not have\", \"shan't\": \"shall not\", \"sha'n't\": \"shall not\", \"shan't've\": \"shall not have\", \"she'd\": \"she would\", \"she'd've\": \"she would have\", \"she'll\": \"she will\", \"she'll've\": \"she will have\", \"she's\": \"she is\", \"should've\": \"should have\", \"shouldn't\": \"should not\", \"shouldn't've\": \"should not have\", \"so've\": \"so have\",\"so's\": \"so as\", \"this's\": \"this is\",\"that'd\": \"that would\", \"that'd've\": \"that would have\", \"that's\": \"that is\", \"there'd\": \"there would\", \"there'd've\": \"there would have\", \"there's\": \"there is\", \"here's\": \"here is\",\"they'd\": \"they would\", \"they'd've\": \"they would have\", \"they'll\": \"they will\", \"they'll've\": \"they will have\", \"they're\": \"they are\", \"they've\": \"they have\", \"to've\": \"to have\", \"wasn't\": \"was not\", \"we'd\": \"we would\", \"we'd've\": \"we would have\", \"we'll\": \"we will\", \"we'll've\": \"we will have\", \"we're\": \"we are\", \"we've\": \"we have\", \"weren't\": \"were not\", \"what'll\": \"what will\", \"what'll've\": \"what will have\", \"what're\": \"what are\",  \"what's\": \"what is\", \"what've\": \"what have\", \"when's\": \"when is\", \"when've\": \"when have\", \"where'd\": \"where did\", \"where's\": \"where is\", \"where've\": \"where have\", \"who'll\": \"who will\", \"who'll've\": \"who will have\", \"who's\": \"who is\", \"who've\": \"who have\", \"why's\": \"why is\", \"why've\": \"why have\", \"will've\": \"will have\", \"won't\": \"will not\", \"won't've\": \"will not have\", \"would've\": \"would have\", \"wouldn't\": \"would not\", \"wouldn't've\": \"would not have\", \"y'all\": \"you all\", \"y'all'd\": \"you all would\",\"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\"y'all've\": \"you all have\",\"you'd\": \"you would\", \"you'd've\": \"you would have\", \"you'll\": \"you will\", \"you'll've\": \"you will have\", \"you're\": \"you are\", \"you've\": \"you have\" }\n",
    "WORDS_TO_REMOVE = ['http', 'https', 'ya']\n",
    "\n",
    "\n",
    "def handle_punctuation(text):\n",
    "    text = text.translate(REMOVE_DICT)\n",
    "    text = text.translate(NEUTRAL_EMOJI_DICT)\n",
    "    text = text.translate(POSITIVE_EMOJI_DICT)\n",
    "    text = text.translate(NEGATIVE_EMOJI_DICT)\n",
    "    text = text.translate(ISOLATE_DICT)\n",
    "    return text\n",
    "\n",
    "def clean_contractions(text, mapping=CONTRACTION_MAPPING):\n",
    "    specials = [\"â€™\", \"â€˜\", \"Â´\", \"`\"]\n",
    "    for s in specials:\n",
    "        text = text.replace(s, \"'\")\n",
    "    text = ' '.join([mapping[t] if t in mapping else t for t in text.split(\" \")])\n",
    "    return text\n",
    "\n",
    "def text_general_reworking(text):\n",
    "    spaces = ['\\u200b', '\\u200e', '\\u202a', '\\u202c', '\\ufeff', '\\uf0d8', '\\u2061', '\\x10', '\\x7f', '\\x9d', '\\xad', '\\xa0']\n",
    "    for space in spaces:\n",
    "        text = text.replace(space, ' ')\n",
    "    text = re.sub('\\s+', ' ', text)\n",
    "    text = re.sub(r'\\.{2,}', ' ', text)\n",
    "    text = re.sub(r'(:\\s?\\)|:-\\)|\\(\\s?:|\\(-:|:\\'\\))', ' positive emoji ', text)\n",
    "    text = re.sub(r'(:\\s?D|:-D|x-?D|X-?D)', ' positive emoji ', text)\n",
    "    text = re.sub(r'(<3|:\\*)', ' positive emoji ', text)\n",
    "    text = re.sub(r'(;-?\\)|;-?D|\\(-?;)', ' positive emoji ', text)\n",
    "    text = re.sub(r'(:\\s?\\(|:-\\(|\\)\\s?:|\\)-:)', ' negative emoji ', text)\n",
    "    text = re.sub(r'(:,\\(|:\\'\\(|:\"\\()', ' negative emoji ', text)\n",
    "    text = re.sub(r'((www\\.[\\S]+)|(https?://[\\S]+))', ' link ', text)\n",
    "    text = re.sub(r'\\brt\\b', '', text)\n",
    "    text = re.sub(r'\\[math\\]', ' LaTex math ', text)\n",
    "    text = re.sub(r'\\[\\/math\\]', ' LaTex math ', text)\n",
    "    text = re.sub(r'\\\\', ' LaTex ', text)\n",
    "    text = re.sub(r'\\brt\\b', '', text)\n",
    "    text = re.sub(\"([^\\\"^\\'].\\s)(\\\")([A-Z,a-z?])\", r\"\\1\\3\", text)\n",
    "    text = re.sub(\"(\\')(.\\\")\", r\"\\2\", text)\n",
    "    text = text.lower()\n",
    "    if '-' in text:\n",
    "        text = text.replace('-', ' - ')\n",
    "    text.strip()\n",
    "    return text\n",
    "\n",
    "def preprocess_text(data):\n",
    "    if pd.isna(data):\n",
    "        return \"\"\n",
    "    #Original preprocessing\n",
    "    words = word_tokenize(data.lower())\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    filtered_words = [word for word in words if word.isalnum() and word not in stop_words]\n",
    "\n",
    "    #Remove a particular words\n",
    "    filtered_words = [word for word in filtered_words if word not in WORDS_TO_REMOVE]\n",
    "    processed_text = \" \".join(filtered_words)\n",
    "\n",
    "    # Additional preprocessing steps\n",
    "    processed_text = text_general_reworking(processed_text)\n",
    "    processed_text = handle_punctuation(processed_text)\n",
    "    processed_text = clean_contractions(processed_text)\n",
    "\n",
    "    return processed_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "###COMMON FEATURES\n",
    "def compute_text_length_features(df, text_column):\n",
    "    df['char_count'] = df[text_column].apply(len)\n",
    "    df['word_count'] = df[text_column].apply(lambda x: len(x.split()))\n",
    "    df['sentence_count'] = df[text_column].apply(lambda x: len([s for s in re.split(r'[.!?]', x) if s.strip() != '']))\n",
    "    df['avg_word_length'] = df['char_count'] / (df['word_count'] + 1) # avoid division by zero\n",
    "    return df\n",
    "\n",
    "\n",
    "def compute_sentiment_features(df, text_column):\n",
    "    sia = SentimentIntensityAnalyzer()\n",
    "    #df['sentiment'] = df[text_column].apply(lambda x: TextBlob(x).sentiment.polarity)\n",
    "    sentiments = df[text_column].apply(lambda  x: sia.polarity_scores(x))\n",
    "    df['sentiment_neg'] = sentiments.apply(lambda x: x['neg'])\n",
    "    df['sentiment_neu'] = sentiments.apply(lambda x: x['neu'])\n",
    "    df['sentiment_pos'] = sentiments.apply(lambda x: x['pos'])\n",
    "    df['sentiment_compound'] = sentiments.apply(lambda x: x['compound'])\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def compute_lexical_diversity(df, text_column):\n",
    "    df['lexical_diversity'] = df[text_column].apply(lambda x: len(set(x.split())) / (len(x.split()) + 1))\n",
    "    return df\n",
    "\n",
    "def compute_subjectivity(df, text_column):\n",
    "    df['subjectivity'] = df[text_column].apply(lambda x: TextBlob(x).sentiment.subjectivity)\n",
    "    return df\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Function to load troll data\n",
    "def load_merged_data(filepath):\n",
    "    try:\n",
    "        df = pd.read_csv(filepath, low_memory=False)\n",
    "        df['processed_content'] = df['text'].apply(preprocess_text)\n",
    "        df = compute_text_length_features(df, 'processed_content')\n",
    "        df = compute_sentiment_features(df, 'processed_content')\n",
    "\n",
    "        #df = compute_word2vec_features(df, 'processed_content')\n",
    "        #df = reduce_dimensionality(df, 'w2v_features')\n",
    "        \n",
    "        #df = compute_stylistic_features(df, 'processed_content')\n",
    "        #df = compute_linguistic_features(df, 'processed_content')\n",
    "        #df = compute_punctuation_features(df, 'processed_content')\n",
    "        df = compute_lexical_diversity(df, 'processed_content')\n",
    "        df = compute_subjectivity(df, 'processed_content')\n",
    "        #df = compute_ngrams_features(df, 'processed_content')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        #CALLING EXCLUSIVE FEATURE FUNCTIONS\n",
    "        #df = compute_interaction_features(df)\n",
    "        #df = compute_temporal_features(df)\n",
    "        \n",
    "        return df\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Failed to load troll data from {filepath}: {e}\")\n",
    "        raise\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "###DISTILBERT NLP\n",
    "def data_generator(data, batch_size):\n",
    "    for start_idx in range(0, len(data), batch_size):\n",
    "        end_idx = min(start_idx + batch_size, len(data))\n",
    "        yield data[start_idx:end_idx]\n",
    "\n",
    "def visualize_tsne(X_text, y=None):\n",
    "    tsne = TSNE(n_components=2, random_state=42)\n",
    "    X_tsne = tsne.fit_transform(X_text)  # Reduce to 2D\n",
    "    \n",
    "    plt.figure(figsize=(8, 6))\n",
    "    if y is not None:  # If you have labels to color the points\n",
    "        plt.scatter(X_tsne[:, 0], X_tsne[:, 1], c=y, cmap='coolwarm', s=10)\n",
    "    else:\n",
    "        plt.scatter(X_tsne[:, 0], X_tsne[:, 1], s=10)\n",
    "    \n",
    "    plt.colorbar()\n",
    "    plt.title('t-SNE of DistilBERT Features')\n",
    "    plt.xlabel('t-SNE component 1')\n",
    "    plt.ylabel('t-SNE component 2')\n",
    "    plt.show()\n",
    "\n",
    "def extract_and_save_distilbert_features(data, batch_size=32, output_file='bert_features10000.h5'):\n",
    "    tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "    model = TFDistilBertModel.from_pretrained('distilbert-base-uncased')\n",
    "    \n",
    "    with h5py.File(output_file, 'w') as hf:\n",
    "        dset = hf.create_dataset(\"features\", (len(data), 768), dtype='float32')\n",
    "\n",
    "        for i, batch in enumerate(data_generator(data, batch_size)):\n",
    "            logging.info(f'Processing batch {i + 1}/{(len(data) + batch_size - 1) // batch_size}')\n",
    "            inputs = tokenizer(batch.tolist(), return_tensors='tf', padding=True, truncation=True, max_length=512)\n",
    "            \n",
    "            outputs = model(inputs)\n",
    "            batch_features = np.mean(outputs.last_hidden_state.numpy(), axis=1)\n",
    "            \n",
    "            dset[i * batch_size: i * batch_size + len(batch)] = batch_features\n",
    "            \n",
    "            gc.collect()\n",
    "    \n",
    "    logging.info('Feature extraction and saving completed')\n",
    "\n",
    "def load_bert_features(filename):\n",
    "    with h5py.File(filename, 'r') as hf:\n",
    "        features = hf['features'][:]\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'def plot_feature_importance(model, feature_names, top_n=20):\\n    importance = model.feature_importances_\\n    indices = np.argsort(importance)[-top_n:]\\n\\n    # Debugging outputs\\n    logging.info(f\"Feature importance values: {importance}\")\\n    logging.info(f\"Top {top_n} feature indices: {indices}\")\\n    logging.info(f\"Top {top_n} feature names: {[feature_names[i] for i in indices]}\")\\n    \\n    plt.figure(figsize=(10, 6))\\n    plt.title(\\'Feature Importance\\')\\n    plt.barh(range(len(indices)), importance[indices], color=\\'b\\', align=\\'center\\')\\n    plt.yticks(range(len(indices)), [feature_names[i] for i in indices])\\n    plt.xlabel(\\'Relative Importance\\')\\n    plt.show()'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#PLOT\n",
    "\n",
    "'''def plot_feature_importance(model, feature_names, top_n=20):\n",
    "    importance = model.feature_importances_\n",
    "    indices = np.argsort(importance)[-top_n:]\n",
    "\n",
    "    # Debugging outputs\n",
    "    logging.info(f\"Feature importance values: {importance}\")\n",
    "    logging.info(f\"Top {top_n} feature indices: {indices}\")\n",
    "    logging.info(f\"Top {top_n} feature names: {[feature_names[i] for i in indices]}\")\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.title('Feature Importance')\n",
    "    plt.barh(range(len(indices)), importance[indices], color='b', align='center')\n",
    "    plt.yticks(range(len(indices)), [feature_names[i] for i in indices])\n",
    "    plt.xlabel('Relative Importance')\n",
    "    plt.show()'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'def train_and_evaluate(X, y):\\n    # Split the data into training and test sets\\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\\n    \\n    # Init the classifier\\n    classifier = KNeighborsClassifier(n_neighbors=5)  # You can adjust n_neighbors as needed\\n    \\n    # Train the classifier\\n    classifier.fit(X_train, y_train)\\n    \\n    # Predict on the test \\n    y_pred = classifier.predict(X_test)\\n    \\n    # Print the metrics\\n    accuracy = accuracy_score(y_test, y_pred)\\n    f1 = f1_score(y_test, y_pred)\\n    precision = precision_score(y_test, y_pred)\\n    recall = recall_score(y_test, y_pred)\\n    roc_auc = roc_auc_score(y_test, y_pred)\\n    \\n    print(\"Accuracy:\", accuracy)\\n    print(\"F1 Score:\", f1)\\n    print(\"Precision:\", precision)\\n    print(\"Recall:\", recall)\\n    print(\"ROC AUC:\", roc_auc)\\n    print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))\\n\\n    #Confusion Matrix\\n    cm = confusion_matrix(y_test, y_pred)\\n    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=classifier.classes_)\\n    disp.plot(cmap=plt.cm.Blues)\\n    plt.title(\"Confusion Matrix\")\\n    plt.show()\\n\\n\\n    return classifier\\n'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "# Vectorize text data\n",
    "def vectorize_text(data):\n",
    "    tfidf_vectorizer = TfidfVectorizer(max_features=10000, stop_words='english')\n",
    "    X = tfidf_vectorizer.fit_transform(data)\n",
    "    return X, tfidf_vectorizer\n",
    "\n",
    "def normalize_features(df, feature_columns):\n",
    "    scaler = MinMaxScaler()\n",
    "    df[feature_columns] = scaler.fit_transform(df[feature_columns])\n",
    "    return df, scaler\n",
    "\n",
    "\n",
    "def combine_features(X_text, df, feature_columns):\n",
    "    X_numerical = df[feature_columns].values\n",
    "    \n",
    "    if X_text.shape[0] != X_numerical.shape[0]:\n",
    "        raise ValueError(f\"Mismatch in number of samples: X_text has {X_text.shape[0]} samples, while X_numerical has {X_numerical.shape[0]} samples.\")\n",
    "    \n",
    "    X = np.hstack((X_text, X_numerical))\n",
    "    return X\n",
    "\n",
    "\n",
    "#ANOTHER METHOD\n",
    "#K-Fold cross validation\n",
    "'''def train_and_evaluate(X, y):\n",
    "    classifier = xgb.XGBClassifier(use_label_encoder=False, eval_metric='logloss', reg_alpha=5, reg_lambda=5, max_depth=4, n_estimators=100)\n",
    "    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    accuracies = []\n",
    "    f1_scores = []\n",
    "    roc_aucs = []\n",
    "    precision_scores = []\n",
    "    recall_scores = []\n",
    "    roc_aucs = []\n",
    "\n",
    "    for train_index, test_index in skf.split(X, y):\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "        \n",
    "        classifier.fit(X_train, y_train)\n",
    "        y_pred = classifier.predict(X_test)\n",
    "        \n",
    "        accuracies.append(accuracy_score(y_test, y_pred))\n",
    "        f1_scores.append(f1_score(y_test, y_pred))\n",
    "        #precision_scores.append(precision_scores(y_test, y_pred))\n",
    "        roc_aucs.append(roc_auc_score(y_test, y_pred))\n",
    "        recall_scores.append(recall_score(y_test, y_pred))\n",
    "        roc_aucs.append(roc_auc_score(y_test, y_pred))\n",
    "        \n",
    "        print(\"Fold Results:\")\n",
    "        print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "        print(\"F1 Score:\", f1_score(y_test, y_pred))\n",
    "        print(\"Precision:\", precision_score(y_test, y_pred))\n",
    "        print(\"Recall:\", recall_score(y_test, y_pred))\n",
    "        print(\"ROC AUC:\", roc_auc_score(y_test, y_pred))\n",
    "        print(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n",
    "\n",
    "    print(\"Average Accuracy:\", np.mean(accuracies))\n",
    "    print(\"Average F1 Score:\", np.mean(f1_scores))\n",
    "    print(\"Average Precision:\", np.mean(precision_scores))\n",
    "    print(\"Average Recall:\", np.mean(recall_scores))\n",
    "    print(\"Average ROC AUC:\", np.mean(roc_aucs))\n",
    "\n",
    "    return classifier'''\n",
    "#K-fold without cross validation XGBOOST\n",
    "def train_and_evaluate(X, y):\n",
    "    # Split the data into training and test sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "    \n",
    "    # Init the classifier\n",
    "    classifier = xgb.XGBClassifier(use_label_encoder=False, eval_metric='logloss', reg_alpha=5, reg_lambda=5, max_depth=4, n_estimators=100)\n",
    "    \n",
    "    # Train the classifier\n",
    "    classifier.fit(X_train, y_train)\n",
    "    \n",
    "    # Predict on the test \n",
    "    y_pred = classifier.predict(X_test)\n",
    "    \n",
    "    # Print the metrics\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred)\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "    roc_auc = roc_auc_score(y_test, y_pred)\n",
    "    \n",
    "    print(\"Accuracy:\", accuracy)\n",
    "    print(\"F1 Score:\", f1)\n",
    "    print(\"Precision:\", precision)\n",
    "    print(\"Recall:\", recall)\n",
    "    print(\"ROC AUC:\", roc_auc)\n",
    "    print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))\n",
    "\n",
    "    #Confusion Matrix\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=classifier.classes_)\n",
    "    disp.plot(cmap=plt.cm.Blues)\n",
    "    plt.title(\"Confusion Matrix\")\n",
    "    plt.show()\n",
    "\n",
    "    return classifier\n",
    "\n",
    "\n",
    "#80/20\n",
    "'''def train_and_evaluate(X, y):\n",
    "    # Split the data into training and testing sets (80/20 split)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "    # Initialize the classifier\n",
    "    classifier = xgb.XGBClassifier(use_label_encoder=False, eval_metric='logloss', reg_alpha=5, reg_lambda=5, max_depth=4, n_estimators=100)\n",
    "    \n",
    "    # Train the classifier on the training set\n",
    "    classifier.fit(X_train, y_train)\n",
    "\n",
    "    # Predict on the test set\n",
    "    y_pred = classifier.predict(X_test)\n",
    "    \n",
    "    # Evaluate the classifier\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred)\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "    roc_auc = roc_auc_score(y_test, y_pred)\n",
    "    \n",
    "    print(\"Results on Test Set:\")\n",
    "    print(\"Accuracy:\", accuracy)\n",
    "    print(\"F1 Score:\", f1)\n",
    "    print(\"Precision:\", precision)\n",
    "    print(\"Recall:\", recall)\n",
    "    print(\"ROC AUC:\", roc_auc)\n",
    "    print(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n",
    "\n",
    "    return classifier'''\n",
    "\n",
    "\n",
    "# SVM\n",
    "'''def train_and_evaluate(X, y):\n",
    "    # Split the data into training and test sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "    \n",
    "    # Init the classifier\n",
    "    classifier = SVC(kernel='linear', probability=True)\n",
    "    \n",
    "    # Train the classifier\n",
    "    classifier.fit(X_train, y_train)\n",
    "    \n",
    "    # Predict on the test \n",
    "    y_pred = classifier.predict(X_test)\n",
    "    \n",
    "    # Print the metrics\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred)\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "    roc_auc = roc_auc_score(y_test, y_pred)\n",
    "    \n",
    "    print(\"Accuracy:\", accuracy)\n",
    "    print(\"F1 Score:\", f1)\n",
    "    print(\"Precision:\", precision)\n",
    "    print(\"Recall:\", recall)\n",
    "    print(\"ROC AUC:\", roc_auc)\n",
    "    print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))\n",
    "\n",
    "    return classifier'''\n",
    "\n",
    "\n",
    "#NAIVE BAYES\n",
    "'''def train_and_evaluate(X, y):\n",
    "    # Split the data into training and test sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "    \n",
    "    # Init the classifier\n",
    "    classifier = GaussianNB()\n",
    "    \n",
    "    # Train the classifier\n",
    "    classifier.fit(X_train, y_train)\n",
    "    \n",
    "    # Predict on the test \n",
    "    y_pred = classifier.predict(X_test)\n",
    "    \n",
    "    # Print the metrics\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred)\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "    roc_auc = roc_auc_score(y_test, y_pred)\n",
    "    \n",
    "    print(\"Accuracy:\", accuracy)\n",
    "    print(\"F1 Score:\", f1)\n",
    "    print(\"Precision:\", precision)\n",
    "    print(\"Recall:\", recall)\n",
    "    print(\"ROC AUC:\", roc_auc)\n",
    "    print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))\n",
    "\n",
    "    return classifier\n",
    "'''\n",
    "#KNN\n",
    "'''def train_and_evaluate(X, y):\n",
    "    # Split the data into training and test sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "    \n",
    "    # Init the classifier\n",
    "    classifier = KNeighborsClassifier(n_neighbors=5)  # You can adjust n_neighbors as needed\n",
    "    \n",
    "    # Train the classifier\n",
    "    classifier.fit(X_train, y_train)\n",
    "    \n",
    "    # Predict on the test \n",
    "    y_pred = classifier.predict(X_test)\n",
    "    \n",
    "    # Print the metrics\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred)\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "    roc_auc = roc_auc_score(y_test, y_pred)\n",
    "    \n",
    "    print(\"Accuracy:\", accuracy)\n",
    "    print(\"F1 Score:\", f1)\n",
    "    print(\"Precision:\", precision)\n",
    "    print(\"Recall:\", recall)\n",
    "    print(\"ROC AUC:\", roc_auc)\n",
    "    print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))\n",
    "\n",
    "    #Confusion Matrix\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=classifier.classes_)\n",
    "    disp.plot(cmap=plt.cm.Blues)\n",
    "    plt.title(\"Confusion Matrix\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "    return classifier\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class distribution:\n",
      "1    50000\n",
      "0    50000\n",
      "Name: label, dtype: int64\n",
      "Accuracy: 0.87285\n",
      "F1 Score: 0.8692881007453097\n",
      "Precision: 0.8943416181914331\n",
      "Recall: 0.8456\n",
      "ROC AUC: 0.8728500000000001\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.90      0.88     10000\n",
      "           1       0.89      0.85      0.87     10000\n",
      "\n",
      "    accuracy                           0.87     20000\n",
      "   macro avg       0.87      0.87      0.87     20000\n",
      "weighted avg       0.87      0.87      0.87     20000\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/ismayilzadamaharram/Desktop/Ameteur_Experience/Bachelor_Work/Notebooks/FreshStrartTraining1Hybrid.ipynb Cell 8'\u001b[0m in \u001b[0;36m<cell line: 57>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ismayilzadamaharram/Desktop/Ameteur_Experience/Bachelor_Work/Notebooks/FreshStrartTraining1Hybrid.ipynb#ch0000007?line=51'>52</a>\u001b[0m y \u001b[39m=\u001b[39m combined_df[\u001b[39m'\u001b[39m\u001b[39mlabel\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mvalues\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ismayilzadamaharram/Desktop/Ameteur_Experience/Bachelor_Work/Notebooks/FreshStrartTraining1Hybrid.ipynb#ch0000007?line=54'>55</a>\u001b[0m classifier \u001b[39m=\u001b[39m train_and_evaluate(X, y)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/ismayilzadamaharram/Desktop/Ameteur_Experience/Bachelor_Work/Notebooks/FreshStrartTraining1Hybrid.ipynb#ch0000007?line=56'>57</a>\u001b[0m visualize_tsne(X_text, y)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ismayilzadamaharram/Desktop/Ameteur_Experience/Bachelor_Work/Notebooks/FreshStrartTraining1Hybrid.ipynb#ch0000007?line=58'>59</a>\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mxgb_model.pkl\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mwb\u001b[39m\u001b[39m'\u001b[39m) \u001b[39mas\u001b[39;00m model_file:\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ismayilzadamaharram/Desktop/Ameteur_Experience/Bachelor_Work/Notebooks/FreshStrartTraining1Hybrid.ipynb#ch0000007?line=59'>60</a>\u001b[0m     pickle\u001b[39m.\u001b[39mdump(classifier, model_file)\n",
      "\u001b[1;32m/Users/ismayilzadamaharram/Desktop/Ameteur_Experience/Bachelor_Work/Notebooks/FreshStrartTraining1Hybrid.ipynb Cell 5'\u001b[0m in \u001b[0;36mvisualize_tsne\u001b[0;34m(X_text, y)\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/ismayilzadamaharram/Desktop/Ameteur_Experience/Bachelor_Work/Notebooks/FreshStrartTraining1Hybrid.ipynb#ch0000004?line=6'>7</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mvisualize_tsne\u001b[39m(X_text, y\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/ismayilzadamaharram/Desktop/Ameteur_Experience/Bachelor_Work/Notebooks/FreshStrartTraining1Hybrid.ipynb#ch0000004?line=7'>8</a>\u001b[0m     tsne \u001b[39m=\u001b[39m TSNE(n_components\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m, random_state\u001b[39m=\u001b[39m\u001b[39m42\u001b[39m)\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/ismayilzadamaharram/Desktop/Ameteur_Experience/Bachelor_Work/Notebooks/FreshStrartTraining1Hybrid.ipynb#ch0000004?line=8'>9</a>\u001b[0m     X_tsne \u001b[39m=\u001b[39m tsne\u001b[39m.\u001b[39;49mfit_transform(X_text)  \u001b[39m# Reduce to 2D\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ismayilzadamaharram/Desktop/Ameteur_Experience/Bachelor_Work/Notebooks/FreshStrartTraining1Hybrid.ipynb#ch0000004?line=10'>11</a>\u001b[0m     plt\u001b[39m.\u001b[39mfigure(figsize\u001b[39m=\u001b[39m(\u001b[39m8\u001b[39m, \u001b[39m6\u001b[39m))\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ismayilzadamaharram/Desktop/Ameteur_Experience/Bachelor_Work/Notebooks/FreshStrartTraining1Hybrid.ipynb#ch0000004?line=11'>12</a>\u001b[0m     \u001b[39mif\u001b[39;00m y \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:  \u001b[39m# If you have labels to color the points\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/_set_output.py:313\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[0;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[1;32m    311\u001b[0m \u001b[39m@wraps\u001b[39m(f)\n\u001b[1;32m    312\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwrapped\u001b[39m(\u001b[39mself\u001b[39m, X, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m--> 313\u001b[0m     data_to_wrap \u001b[39m=\u001b[39m f(\u001b[39mself\u001b[39;49m, X, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    314\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(data_to_wrap, \u001b[39mtuple\u001b[39m):\n\u001b[1;32m    315\u001b[0m         \u001b[39m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[1;32m    316\u001b[0m         return_tuple \u001b[39m=\u001b[39m (\n\u001b[1;32m    317\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[39m0\u001b[39m], X, \u001b[39mself\u001b[39m),\n\u001b[1;32m    318\u001b[0m             \u001b[39m*\u001b[39mdata_to_wrap[\u001b[39m1\u001b[39m:],\n\u001b[1;32m    319\u001b[0m         )\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/sklearn/base.py:1473\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1466\u001b[0m     estimator\u001b[39m.\u001b[39m_validate_params()\n\u001b[1;32m   1468\u001b[0m \u001b[39mwith\u001b[39;00m config_context(\n\u001b[1;32m   1469\u001b[0m     skip_parameter_validation\u001b[39m=\u001b[39m(\n\u001b[1;32m   1470\u001b[0m         prefer_skip_nested_validation \u001b[39mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1471\u001b[0m     )\n\u001b[1;32m   1472\u001b[0m ):\n\u001b[0;32m-> 1473\u001b[0m     \u001b[39mreturn\u001b[39;00m fit_method(estimator, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/sklearn/manifold/_t_sne.py:1176\u001b[0m, in \u001b[0;36mTSNE.fit_transform\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m   1173\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_max_iter \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmax_iter\n\u001b[1;32m   1175\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_params_vs_input(X)\n\u001b[0;32m-> 1176\u001b[0m embedding \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fit(X)\n\u001b[1;32m   1177\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membedding_ \u001b[39m=\u001b[39m embedding\n\u001b[1;32m   1178\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membedding_\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/sklearn/manifold/_t_sne.py:1044\u001b[0m, in \u001b[0;36mTSNE._fit\u001b[0;34m(self, X, skip_num_points)\u001b[0m\n\u001b[1;32m   1038\u001b[0m \u001b[39m# Degrees of freedom of the Student's t-distribution. The suggestion\u001b[39;00m\n\u001b[1;32m   1039\u001b[0m \u001b[39m# degrees_of_freedom = n_components - 1 comes from\u001b[39;00m\n\u001b[1;32m   1040\u001b[0m \u001b[39m# \"Learning a Parametric Embedding by Preserving Local Structure\"\u001b[39;00m\n\u001b[1;32m   1041\u001b[0m \u001b[39m# Laurens van der Maaten, 2009.\u001b[39;00m\n\u001b[1;32m   1042\u001b[0m degrees_of_freedom \u001b[39m=\u001b[39m \u001b[39mmax\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_components \u001b[39m-\u001b[39m \u001b[39m1\u001b[39m, \u001b[39m1\u001b[39m)\n\u001b[0;32m-> 1044\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_tsne(\n\u001b[1;32m   1045\u001b[0m     P,\n\u001b[1;32m   1046\u001b[0m     degrees_of_freedom,\n\u001b[1;32m   1047\u001b[0m     n_samples,\n\u001b[1;32m   1048\u001b[0m     X_embedded\u001b[39m=\u001b[39;49mX_embedded,\n\u001b[1;32m   1049\u001b[0m     neighbors\u001b[39m=\u001b[39;49mneighbors_nn,\n\u001b[1;32m   1050\u001b[0m     skip_num_points\u001b[39m=\u001b[39;49mskip_num_points,\n\u001b[1;32m   1051\u001b[0m )\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/sklearn/manifold/_t_sne.py:1112\u001b[0m, in \u001b[0;36mTSNE._tsne\u001b[0;34m(self, P, degrees_of_freedom, n_samples, X_embedded, neighbors, skip_num_points)\u001b[0m\n\u001b[1;32m   1110\u001b[0m     opt_args[\u001b[39m\"\u001b[39m\u001b[39mmomentum\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39m0.8\u001b[39m\n\u001b[1;32m   1111\u001b[0m     opt_args[\u001b[39m\"\u001b[39m\u001b[39mn_iter_without_progress\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_iter_without_progress\n\u001b[0;32m-> 1112\u001b[0m     params, kl_divergence, it \u001b[39m=\u001b[39m _gradient_descent(obj_func, params, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mopt_args)\n\u001b[1;32m   1114\u001b[0m \u001b[39m# Save the final number of iterations\u001b[39;00m\n\u001b[1;32m   1115\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_iter_ \u001b[39m=\u001b[39m it\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/sklearn/manifold/_t_sne.py:403\u001b[0m, in \u001b[0;36m_gradient_descent\u001b[0;34m(objective, p0, it, max_iter, n_iter_check, n_iter_without_progress, momentum, learning_rate, min_gain, min_grad_norm, verbose, args, kwargs)\u001b[0m\n\u001b[1;32m    400\u001b[0m \u001b[39m# only compute the error when needed\u001b[39;00m\n\u001b[1;32m    401\u001b[0m kwargs[\u001b[39m\"\u001b[39m\u001b[39mcompute_error\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m check_convergence \u001b[39mor\u001b[39;00m i \u001b[39m==\u001b[39m max_iter \u001b[39m-\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m--> 403\u001b[0m error, grad \u001b[39m=\u001b[39m objective(p, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    405\u001b[0m inc \u001b[39m=\u001b[39m update \u001b[39m*\u001b[39m grad \u001b[39m<\u001b[39m \u001b[39m0.0\u001b[39m\n\u001b[1;32m    406\u001b[0m dec \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39minvert(inc)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/sklearn/manifold/_t_sne.py:280\u001b[0m, in \u001b[0;36m_kl_divergence_bh\u001b[0;34m(params, P, degrees_of_freedom, n_samples, n_components, angle, skip_num_points, verbose, compute_error, num_threads)\u001b[0m\n\u001b[1;32m    277\u001b[0m X_embedded \u001b[39m=\u001b[39m params\u001b[39m.\u001b[39mreshape(n_samples, n_components)\n\u001b[1;32m    279\u001b[0m val_P \u001b[39m=\u001b[39m P\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mastype(np\u001b[39m.\u001b[39mfloat32, copy\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[0;32m--> 280\u001b[0m neighbors \u001b[39m=\u001b[39m P\u001b[39m.\u001b[39;49mindices\u001b[39m.\u001b[39;49mastype(np\u001b[39m.\u001b[39;49mint64, copy\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n\u001b[1;32m    281\u001b[0m indptr \u001b[39m=\u001b[39m P\u001b[39m.\u001b[39mindptr\u001b[39m.\u001b[39mastype(np\u001b[39m.\u001b[39mint64, copy\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[1;32m    283\u001b[0m grad \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mzeros(X_embedded\u001b[39m.\u001b[39mshape, dtype\u001b[39m=\u001b[39mnp\u001b[39m.\u001b[39mfloat32)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Execution block\n",
    "merged_file = \"../Merged_troll_non_troll/merged_dataset100000.csv\"\n",
    "\n",
    "\n",
    "# Load troll and non-troll datasets\n",
    "combined_df = load_merged_data(merged_file)\n",
    "\n",
    "\n",
    "\n",
    "# Investigate class distribution\n",
    "print(\"Class distribution:\")\n",
    "print(combined_df['label'].value_counts())\n",
    "\n",
    "# Define text and numerical feature columns\n",
    "text_data = combined_df['processed_content']\n",
    "numerical_features = ['char_count', 'word_count', 'sentence_count', 'avg_word_length',\n",
    "                      'sentiment_neg', 'sentiment_neu', 'sentiment_pos', 'sentiment_compound',\n",
    "                      'lexical_diversity', 'subjectivity']\n",
    "assert len(text_data) == len(combined_df), \"Length of text data and combined DataFrame do not match\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Vectorize text data\n",
    "#X_text, vectorizer = vectorize_text(text_data)\n",
    "\n",
    "\n",
    "\n",
    "#X_text = extract_distilbert_features(text_data, batch_size=16)\n",
    "\n",
    "# Extract and save DistilBERT features\n",
    "#extract_and_save_distilbert_features(text_data, batch_size=32, output_file='bert_features100000.h5')\n",
    "\n",
    "# Load DistilBERT features from disk\n",
    "X_text = load_bert_features('bert_features100000.h5')\n",
    "\n",
    "# Handle NaN values if present by filling with zeros\n",
    "combined_df[numerical_features] = combined_df[numerical_features].fillna(0)\n",
    "\n",
    "\n",
    "#Normalize num features\n",
    "combined_df, scaler = normalize_features(combined_df, numerical_features)\n",
    "\n",
    "#Combine text and num features\n",
    "X = combine_features(X_text, combined_df, numerical_features)\n",
    "y = combined_df['label'].values\n",
    "\n",
    "\n",
    "classifier = train_and_evaluate(X, y)\n",
    "\n",
    "visualize_tsne(X_text, y)\n",
    "\n",
    "with open('xgb_model.pkl', 'wb') as model_file:\n",
    "    pickle.dump(classifier, model_file)\n",
    "with open('scaler.pkl', 'wb') as scaler_file:\n",
    "    pickle.dump(scaler, scaler_file)\n",
    "\n",
    "\n",
    "# Since DISTILBERT features are not interpretable in the same way as other features, we skip this step for BERT features\n",
    "#feature_names = [f'bert_feature_{i}' for i in range(X_text.shape[1])] + numerical_features\n",
    "#plot_feature_importance(classifier, feature_names)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "72229e2cb4a4d3f9a66c0f3bbf8721a4f899e8fc91e357b565a53efd017c27d6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
